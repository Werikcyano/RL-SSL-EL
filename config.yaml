score_average_over: 100
timesteps_total: 160000000 # Controla numero maximo de timesteps
checkpoint_freq: 10 # Salvar checkpoint a cada 100 iterações 
checkpoint_restore: "/root/ray_results/PPO_selfplay_rec/PPO_Soccer_899ff_00000_0_2025-02-25_19-27-23/checkpoint_000000"

rllib:
  num_cpus: 7  # Depende do num_workers, tem que ser igual ou maior
  num_gpus: 1
  num_workers: 6  # Número de ambientes em paralelo, quanto maior, mais rápido
  num_envs_per_worker: 2 # Número de ambientes por worker, quanto maior, mais rápido
  #num_gpus_per_worker: 0.2 # fração de GPU usado por cada worker (< 1/num_workers), pode melhorar a performance, mas tem que investigar mais
  framework: "torch"
  disable_env_checking: true
PPO:
  batch_mode: "truncate_episodes"
  rollout_fragment_length: "auto"
  train_batch_size: 38520 #workers*envs*fragment
  sgd_minibatch_size: 12840 #batch/3
  gamma: 0.99
  lambda: 0.95
  entropy_coeff: 0.01
  #entropy_coeff_schedule: [[0, 1], [1000000, 0.01]]
  kl_coeff: 0.0
  lr: 0.0004
  vf_loss_coeff: 0.5
  grad_clip: 0.5 # deve ajudar a resolver o problema dos NaNs no pesos da rede
  num_sgd_iter: 5
  clip_param: 0.2
  vf_clip_param: 100000.0 # essentially turning vf_clip off
  normalize_actions: false

evaluation:
  evaluation_interval: 1
  evaluation_num_workers: 0
  evaluation_duration: 1
  evaluation_duration_unit: "episodes"
  evaluation_config:
    env: "Soccer_recorder"
    num_envs_per_worker: 1
  
custom_model:
  fcnet_hiddens: [300, 200, 100]
  vf_share_layers: false
env:
  init_pos: # posição inicial dos jogadores
    blue:
      1: [-0.5,  0.1,    0.0]  # Atacante (leve deslocamento para direita)
      2: [-1.0,  0.0,    0.0]  # Intermediário (centro)
      3: [-1.5, -0.1,    0.0]  # Goleiro (leve deslocamento para esquerda)
    yellow: #{}
      1: [ 0.5,  0.0,  180.0]  # Apenas um oponente, mais próximo da bola
    ball: [0, 0]
  field_type: 0
  fps: 30 # frames por segundo
  match_time: 40 # duração da partida em segundos
  render_mode: "human"
curriculum:
  enabled: true  # Ativando o curriculum learning
  initial_task: 1  # Começando diretamente na tarefa 1
  promotion_threshold: 0.8  # Taxa de sucesso necessária para avançar
  evaluation_window: 100  # Número de episódios para avaliar o desempenho
  tasks:
    0:  # Tarefa básica - Centro de treinamento
      max_steps: 300
      success_distance: 0.2  # Distância considerada sucesso
      reward_touch: 10.0  # Recompensa por tocar na bola
      num_agents_blue: 3  # Atacante, intermediário e goleiro
      num_agents_yellow: 0  # Sem oponentes no nível 0
    1:  # Tarefa intermediária - manter posse contra um oponente
      max_steps: 400
      success_distance: 0.2
      num_agents_blue: 3
      num_agents_yellow: 1  # Explicitamente definindo 1 robô amarelo
      reward_weights:  # Pesos para diferentes componentes da recompensa
        ball_proximity: 0.1  # Peso para proximidade da bola
        possession: 0.1  # Peso para posse de bola
        opponent_distance: 0.05  # Peso para manter bola longe do oponente
        opponent_penalty: 0.2  # Penalidade quando oponente está mais próximo
      possession_threshold: 0.2  # Distância para considerar posse de bola
      possession_time_success: 90  # Frames necessários com posse (3s * 30fps)
      init_pos:
        blue:  # Posições base para variação aleatória
          1: [-0.5,  0.1,    0.0]  # Atacante
          2: [-1.0,  0.0,    0.0]  # Intermediário
          3: [-1.5, -0.1,    0.0]  # Goleiro
        yellow:
          1: [ 0.5,  0.0,  180.0]  # Oponente mais próximo da bola
        ball: [0, 0]  # Posição inicial da bola (será variada no reset)
    2:  # Tarefa avançada - com obstáculo
      max_steps: 500
      success_distance: 0.2
      num_agents_blue: 3
      num_agents_yellow: 3
      init_pos:
        yellow:
          1: [ 1.0,  0.0,  180.0]
          2: [ 2.0,  1.0,  180.0]
          3: [ 2.0, -1.0,  180.0]