%% NOTE: \LaTeX's comment character `%' is not a comment character in
%% the database files.  \BibTeX allows in the database files any
%% comment that's not within an entry.
%%
%% Repositorio de arquivos .bib disponiveis on-line:
%% http://liinwww.ira.uka.de/bibliography/index.html
%% http://www.math.utah.edu/~beebe/bibliographies.html
%%
%% Se voc� tiver problemas com acentos nas entradas do BibTeX, coloque-os
%% entre { e }, como em Jo{�}o ou ainda Jo{\~a}o.

%---------------------------------------------------------
% A A A
%---------------------------------------------------------
@article{ac_rl_intersections,
author = {Annaswamy, Anuradha M.},
title = {Adaptive Control and Intersections with Reinforcement Learning},
journal = {Annual Review of Control, Robotics, and Autonomous Systems},
volume = {6},
number = {1},
pages = {65-93},
year = {2023},
doi = {10.1146/annurev-control-062922-090153},
URL = { 
        https://doi.org/10.1146/annurev-control-062922-090153
},
eprint = { 
        https://doi.org/10.1146/annurev-control-062922-090153
    }
}

@book{controle_adaptativo_livro,
  title={Adaptive control},
  author={{\AA}str{\"o}m, Karl J and Wittenmark, Bj{\"o}rn},
  year={2013},
  publisher={Courier Corporation}
}



@InProceedings{relay_long_horizon,
  title = 	 {Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning},
  author =       {Gupta, Abhishek and Kumar, Vikash and Lynch, Corey and Levine, Sergey and Hausman, Karol},
  booktitle = 	 {Proceedings of the Conference on Robot Learning},
  pages = 	 {1025--1037},
  year = 	 {2020},
  editor = 	 {Kaelbling, Leslie Pack and Kragic, Danica and Sugiura, Komei},
  volume = 	 {100},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Oct--01 Nov},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v100/gupta20a/gupta20a.pdf},
  url = 	 {https://proceedings.mlr.press/v100/gupta20a.html},
  abstract = 	 {We present relay policy learning, a method for imitation and reinforcement learning that can solve multi-stage, long-horizon robotic tasks. This general and universally-applicable, two-phase approach consists of an imitation learning stage resulting in goal-conditioned hierarchical policies that can be easily improved using fine-tuning via reinforcement learning in the subsequent phase. Our method, while not necessarily perfect at imitation learning, is very amenable to further improvement via environment interaction allowing it to scale to challenging long-horizon tasks. In particular, we simplify the long-horizon policy learning problem by using a novel data-relabeling algorithm for learning goal-conditioned hierarchical policies, where the low-level only acts for a fixed number of steps, regardless of the goal achieved. While we rely on demonstration data to bootstrap policy learning, we do not assume access to demonstrations of specific tasks. Instead, our approach can leverage unstructured and unsegmented demonstrations of semantically meaningful behaviors that are not only less burdensome to provide, but also can greatly facilitate further improvement using reinforcement learning. We demonstrate the effectiveness of our method on a number of multi-stage, long-horizon manipulation tasks in a challenging kitchen simulation environment.}
}


@article{ac_teoria_aplicacoes,
  title={Adaptive control theory and applications},
  author={Cao, Chengyu and Ma, Lili and Xu, Yunjun},
  journal={Journal of Control Science and Engineering},
  volume={2012},
  pages={9--9},
  year={2012},
  publisher={Hindawi Limited London, UK, United Kingdom}
}

@book{monte_carlo_statistical_methods,
  title={Monte Carlo statistical methods},
  author={Robert, Christian P and Casella, George and Casella, George},
  volume={2},
  year={1999},
  publisher={Springer}
}


@book{introducao_modelos_probabilisticos,
  title={Introduction to probability models},
  author={Ross, Sheldon M},
  year={2014},
  publisher={Academic press}
}

@article{robocin,
  author       = {Hansenclever F. Bassani and
                  Renie A. Delgado and
                  Jos{\'{e}} Nilton de O. Lima Junior and
                  Heitor R. Medeiros and
                  Pedro H. M. Braga and
                  Alain Tapp},
  title        = {Learning to Play Soccer by Reinforcement and Applying Sim-to-Real
                  to Compete in the Real World},
  journal      = {CoRR},
  volume       = {abs/2003.11102},
  year         = {2020},
  url          = {https://arxiv.org/abs/2003.11102},
  eprinttype    = {arXiv},
  eprint       = {2003.11102},
  timestamp    = {Wed, 16 Sep 2020 09:49:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2003-11102.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{mcmc,
  title={Markov chain Monte Carlo: stochastic simulation for Bayesian inference},
  author={Gamerman, Dani and Lopes, Hedibert F},
  year={2006},
  publisher={CRC press}
}

@ARTICLE{bruno_brandao,
  author={Brandão, Bruno and De Lima, Telma Woerle and Soares, Anderson and Melo, Luckeciano and Maximo, Marcos R. O. A.},
  journal={IEEE Access}, 
  title={Multiagent Reinforcement Learning for Strategic Decision Making and Control in Robotic Soccer Through Self-Play}, 
  year={2022},
  volume={10},
  number={},
  pages={72628-72642},
  doi={10.1109/ACCESS.2022.3189021}}

@article{dream_v1,
  author       = {Danijar Hafner and
                  Timothy P. Lillicrap and
                  Jimmy Ba and
                  Mohammad Norouzi},
  title        = {Dream to Control: Learning Behaviors by Latent Imagination},
  journal      = {CoRR},
  volume       = {abs/1912.01603},
  year         = {2019},
  url          = {http://arxiv.org/abs/1912.01603},
  eprinttype    = {arXiv},
  eprint       = {1912.01603},
  timestamp    = {Tue, 07 Jan 2020 13:36:12 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1912-01603.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{dreamer_v2,
  author       = {Danijar Hafner and
                  Timothy P. Lillicrap and
                  Mohammad Norouzi and
                  Jimmy Ba},
  title        = {Mastering Atari with Discrete World Models},
  journal      = {CoRR},
  volume       = {abs/2010.02193},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.02193},
  eprinttype    = {arXiv},
  eprint       = {2010.02193},
  timestamp    = {Mon, 12 Oct 2020 17:53:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-02193.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{dremaer_v3,
      title={Mastering Diverse Domains through World Models}, 
      author={Danijar Hafner and Jurgis Pasukonis and Jimmy Ba and Timothy Lillicrap},
      year={2023},
      eprint={2301.04104},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@book{sutton,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{46_mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ international conference on intelligent robots and systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@inproceedings{44_comparacao_mujoco,
  title={Simulation tools for model-based robotics: Comparison of bullet, havok, mujoco, ode and physx},
  author={Erez, Tom and Tassa, Yuval and Todorov, Emanuel},
  booktitle={2015 IEEE international conference on robotics and automation (ICRA)},
  pages={4397--4404},
  year={2015},
  organization={IEEE}
}

@article{5_openai_gym,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@inproceedings{11_selecao_acoes,
  title={Using reinforcement learning techniques to select the best action in setplays with multiple possibilities in robocup soccer simulation teams},
  author={Fabro, Jo{\~a}o A and Reis, Luis P and Lau, Nuno},
  booktitle={2014 joint conference on robotics: SBR-LARS robotics symposium and robocontrol},
  pages={85--90},
  year={2014},
  organization={IEEE}
}

@article{pequi_tdp,
  title={Pequi Mec{\^a}nico IEEE VSS Soccer Team-CBR 2017},
  author={Gomes, Adriel O and Paula, Alisson R and Martins, Bruno BS and Oliveira, Bryan LM and Silva, Daniel F and Quijano, Eduardo HD and Assis, Lucas S and Dias, Nigel JB and Mortosa, Ot{\'a}vio S and Alves, Pedro SR and others}
}

@misc{cbr_site,
    title = {Latin American and Brazilian Robotics Competition},
    year = {2023},
    url = {https://www.cbrobotica.org/}
}

@misc{regras_vss2023,
    title = {Regras para competição IEEE Very Small Competition},
    year = {2023},
    url = {http://www.cbrobotica.org/wp-content/uploads/2023/04/regrasVSS23.pdf}
}

@article{trabalho_thiago,
    author = {De Oliveira, T. H.},
    title = {Fusão de sensores para robô da categoria VSSS},
    year = {2018},
    type = {Bachelors Thesis},
    school = {Universidade Federal de Goias}
}

@inproceedings{robocup,
    author = {Fahami, M. A. and Roshanzamir, M. and Izadi, N. H.},
    title = {A Reinforcement Learning Approach to Score Goals in RoboCup 3D Soccer Simulation for NAO Humanoid Robot},
    booktitle = {2017 7th International Conference on Computer and Knowledge Engineering (ICCKE)},
    pages = {450--454},
    year = {2017},
    month = {2},
    day = {26}
}


@book{lapan2018,
    author = {Lapan, M.},
    title = {Deep Reinforcement Learning Hands-On: Apply Modern RL Methods, with Deep Q-Networks, Value Iteration, Policy Gradients, TRPO, AlphaGo Zero and More},
    publisher = {Packt Publishing},
    year = {2018},
    series = {Expert insight}
}

@article{long_short,
  author       = {Ruosong Wang and
                  Simon S. Du and
                  Lin F. Yang and
                  Sham M. Kakade},
  title        = {Is Long Horizon Reinforcement Learning More Difficult Than Short Horizon
                  Reinforcement Learning?},
  journal      = {CoRR},
  volume       = {abs/2005.00527},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.00527},
  eprinttype    = {arXiv},
  eprint       = {2005.00527},
  timestamp    = {Fri, 08 May 2020 15:04:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-00527.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{model_based,
  author       = {Thomas M. Moerland and
                  Joost Broekens and
                  Catholijn M. Jonker},
  title        = {Model-based Reinforcement Learning: {A} Survey},
  journal      = {CoRR},
  volume       = {abs/2006.16712},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.16712},
  eprinttype    = {arXiv},
  eprint       = {2006.16712},
  timestamp    = {Thu, 02 Jul 2020 14:42:48 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-16712.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{modeling_long_horizon,
  title={Modeling long-horizon tasks as sequential interaction landscapes},
  author={Pirk, S{\"o}ren and Hausman, Karol and Toshev, Alexander and Khansari, Mohi},
  journal={arXiv preprint arXiv:2006.04843},
  year={2020}
}


@article{ac_multiple_models,
title = {Adaptive Control Using Collective Information Obtained from Multiple Models*},
journal = {IFAC Proceedings Volumes},
volume = {44},
number = {1},
pages = {362-367},
year = {2011},
note = {18th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20110828-6-IT-1002.02237},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016436372},
author = {Kumpati S Narendra and Zhuo Han},
keywords = {Adaptive control, Adaptive systems, Linear systems, Convex hull, Second level adaptation},
abstract = {Abstract
A radically new approach to adaptive control using multiple models was recently proposed by the authors. Using parametric estimates and outputs generated by multiple adaptive identification models, an adaptive control procedure was proposed which resulted in significantly faster and more accurate response of the overall system, as compared to existing methods such as “switching” and “switching and tuning”, which also use multiple models. In this paper the ideas are extended to include fixed identification models. Further, many of the theoretical questions that have arisen concerning the stability of the overall system and the reasons for the improved performance are addressed in detail. Simulation results are included, towards the end of the paper, to indicate the effectiveness of the new methodology.}
}


@article{multi_ac_survey,
title = {Multivariable adaptive control: A survey},
journal = {Automatica},
volume = {50},
number = {11},
pages = {2737-2764},
year = {2014},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2014.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0005109814003963},
author = {Gang Tao},
keywords = {Adaptive control, Backstepping control, Fault-tolerant control, Certainty equivalence principle, Feedback linearization, Gain matrix decomposition, Multivariable systems, Model reference control, Parameter estimation, Parametrization, Plant-model matching, Pole placement control, Robustness, Stability and tracking, System uncertainties},
abstract = {Adaptive control is a control methodology capable of dealing with uncertain systems to ensure desired control performance. This paper provides an overview of some fundamental theoretical aspects and technical issues of multivariable adaptive control, and a thorough presentation of various adaptive control schemes for multi-input–multi-output systems, literature reviews on adaptive control foundations and multivariable adaptive control methods, and related technical problems. It covers some basic concepts and issues such as certainty equivalence, stability, tracking, robustness, and parameter convergence. It discusses some of the most important topics of adaptive control: plant uncertainty parametrization, stable controller adaptation, and design conditions for different adaptive control schemes. The paper also presents a detailed study of well-developed multivariable model reference adaptive control theory and design techniques. It provides an introduction to multivariable adaptive pole placement and adaptive nonlinear control, and it concludes by identifying some open research problems.}
}


@article{sequential_monte_carlo,
    author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
    title = "{Sequential Monte Carlo Samplers}",
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {68},
    number = {3},
    pages = {411-436},
    year = {2006},
    month = {05},
    abstract = "{We propose a methodology to sample sequentially from a sequence of probability distributions that are defined on a common space, each distribution being known up to a normalizing constant. These probability distributions are approximated by a cloud of weighted random samples which are propagated over time by using sequential Monte Carlo methods. This methodology allows us to derive simple algorithms to make parallel Markov chain Monte Carlo algorithms interact to perform global optimization and sequential Bayesian estimation and to compute ratios of normalizing constants. We illustrate these algorithms for various integration tasks arising in the context of Bayesian inference.}",
    issn = {1369-7412},
    doi = {10.1111/j.1467-9868.2006.00553.x},
    url = {https://doi.org/10.1111/j.1467-9868.2006.00553.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/68/3/411/49795343/jrsssb\_68\_3\_411.pdf},
}

@article{markov_simple,
    author = {van Ravenzwaaij, Don and Cassey, Pete and Brown, Scott D.},
    title = {A simple introduction to Markov Chain Monte–Carlo sampling},
    journal = {Psychonomic Bulletin \& Review},
    volume = {25},
    number = {1},
    pages = {143--154},
    year = {2018},
    month = {2},
    doi = {10.3758/s13423-016-1015-8},
    url = {https://doi.org/10.3758/s13423-016-1015-8},
    issn = {1531-5320}
}

@INPROCEEDINGS{dreaming,
  author={Okada, Masashi and Taniguchi, Tadahiro},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction}, 
  year={2021},
  volume={},
  number={},
  pages={4209-4215},
  doi={10.1109/ICRA48506.2021.9560734}}

@InProceedings{long_horizon,
  title = 	 {Model-Based Reinforcement Learning via Latent-Space Collocation},
  author =       {Rybkin, Oleh and Zhu, Chuning and Nagabandi, Anusha and Daniilidis, Kostas and Mordatch, Igor and Levine, Sergey},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9190--9201},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/rybkin21b/rybkin21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/rybkin21b.html},
  abstract = 	 {The ability to plan into the future while utilizing only raw high-dimensional observations, such as images, can provide autonomous agents with broad and general capabilities. However, realistic tasks require performing temporally extended reasoning, and cannot be solved with only myopic, short-sighted planning. Recent work in model-based reinforcement learning (RL) has shown impressive results on tasks that require only short-horizon reasoning. In this work, we study how the long-horizon planning abilities can be improved with an algorithm that optimizes over sequences of states, rather than actions, which allows better credit assignment. To achieve this, we draw on the idea of collocation and adapt it to the image-based setting by leveraging probabilistic latent variable models, resulting in an algorithm that optimizes trajectories over latent variables. Our latent collocation method (LatCo) provides a general and effective visual planning approach, and significantly outperforms prior model-based approaches on challenging visual control tasks with sparse rewards and long-term goals. See the videos on the supplementary website \url{https://sites.google.com/view/latco-mbrl/.}}
}

@book{eficiencia_amostragem,
    author    = {Max Lapan},
    title     = {Deep Reinforcement Learning Hands-On: Apply modern RL methods, with deep Q-networks, value iteration, policy gradients, TRPO, AlphaGo Zero and more},
    year      = {2018},
    publisher = {Packt Publishing Ltd}
}