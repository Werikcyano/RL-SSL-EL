%% NOTE: \LaTeX's comment character `%' is not a comment character in
%% the database files.  \BibTeX allows in the database files any
%% comment that's not within an entry.
%%
%% Repositorio de arquivos .bib disponiveis on-line:
%% http://liinwww.ira.uka.de/bibliography/index.html
%% http://www.math.utah.edu/~beebe/bibliographies.html
%%
%% Se voc� tiver problemas com acentos nas entradas do BibTeX, coloque-os
%% entre { e }, como em Jo{�}o ou ainda Jo{\~a}o.

%---------------------------------------------------------
% A A A
%---------------------------------------------------------



@book{monte_carlo_statistical_methods,
  title={Monte Carlo statistical methods},
  author={Robert, Christian P and Casella, George and Casella, George},
  volume={2},
  year={1999},
  publisher={Springer}
}


@book{introducao_modelos_probabilisticos,
  title={Introduction to probability models},
  author={Ross, Sheldon M},
  year={2014},
  publisher={Academic press}
}

@article{robocin_trabalho,
  author       = {Hansenclever F. Bassani and
                  Renie A. Delgado and
                  Jos{\'{e}} Nilton de O. Lima Junior and
                  Heitor R. Medeiros and
                  Pedro H. M. Braga and
                  Alain Tapp},
  title        = {Learning to Play Soccer by Reinforcement and Applying Sim-to-Real
                  to Compete in the Real World},
  journal      = {CoRR},
  volume       = {abs/2003.11102},
  year         = {2020},
  url          = {https://arxiv.org/abs/2003.11102},
  eprinttype    = {arXiv},
  eprint       = {2003.11102},
  timestamp    = {Wed, 16 Sep 2020 09:49:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2003-11102.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{mcmc,
  title={Markov chain Monte Carlo: stochastic simulation for Bayesian inference},
  author={Gamerman, Dani and Lopes, Hedibert F},
  year={2006},
  publisher={CRC press}
}

@ARTICLE{bruno_brandao,
  author={Brandão, Bruno and De Lima, Telma Woerle and Soares, Anderson and Melo, Luckeciano and Maximo, Marcos R. O. A.},
  journal={IEEE Access}, 
  title={Multiagent Reinforcement Learning for Strategic Decision Making and Control in Robotic Soccer Through Self-Play}, 
  year={2022},
  volume={10},
  number={},
  pages={72628-72642},
  doi={10.1109/ACCESS.2022.3189021}}



@book{sutton,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}


@article{pequi_tdp,
  title={Pequi Mec{\^a}nico IEEE VSS Soccer Team-CBR 2017},
  author={Gomes, Adriel O and Paula, Alisson R and Martins, Bruno BS and Oliveira, Bryan LM and Silva, Daniel F and Quijano, Eduardo HD and Assis, Lucas S and Dias, Nigel JB and Mortosa, Ot{\'a}vio S and Alves, Pedro SR and others}
}

@misc{cbr_site,
    title = {Latin American and Brazilian Robotics Competition},
    year = {2023},
    url = {https://www.cbrobotica.org/}
}

@misc{regras_ssl_el_2024,
    title = {Regras para competição Robocup Small Size League Entry-Level (SSL-EL) },
    year = {2024},
    url = {https://cbr.robocup.org.br/wp-content/uploads/2024/08/sslrules.pdf},
    howpublished = {\url{https://cbr.robocup.org.br/wp-content/uploads/2024/08/sslrules.pdf}}
}


@inproceedings{robocup,
    author = {Fahami, M. A. and Roshanzamir, M. and Izadi, N. H.},
    title = {A Reinforcement Learning Approach to Score Goals in RoboCup 3D Soccer Simulation for NAO Humanoid Robot},
    booktitle = {2017 7th International Conference on Computer and Knowledge Engineering (ICCKE)},
    pages = {450--454},
    year = {2017},
    month = {2},
    day = {26}
}


@article{sequential_monte_carlo,
    author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
    title = "{Sequential Monte Carlo Samplers}",
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {68},
    number = {3},
    pages = {411-436},
    year = {2006},
    month = {05},
    abstract = "{We propose a methodology to sample sequentially from a sequence of probability distributions that are defined on a common space, each distribution being known up to a normalizing constant. These probability distributions are approximated by a cloud of weighted random samples which are propagated over time by using sequential Monte Carlo methods. This methodology allows us to derive simple algorithms to make parallel Markov chain Monte Carlo algorithms interact to perform global optimization and sequential Bayesian estimation and to compute ratios of normalizing constants. We illustrate these algorithms for various integration tasks arising in the context of Bayesian inference.}",
    issn = {1369-7412},
    doi = {10.1111/j.1467-9868.2006.00553.x},
    url = {https://doi.org/10.1111/j.1467-9868.2006.00553.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/68/3/411/49795343/jrsssb\_68\_3\_411.pdf},
}

@article{markov_simple,
    author = {van Ravenzwaaij, Don and Cassey, Pete and Brown, Scott D.},
    title = {A simple introduction to Markov Chain Monte–Carlo sampling},
    journal = {Psychonomic Bulletin \& Review},
    volume = {25},
    number = {1},
    pages = {143--154},
    year = {2018},
    month = {2},
    doi = {10.3758/s13423-016-1015-8},
    url = {https://doi.org/10.3758/s13423-016-1015-8},
    issn = {1531-5320}
}

@book{eficiencia_amostragem,
    author    = {Max Lapan},
    title     = {Deep Reinforcement Learning Hands-On: Apply modern RL methods, with deep Q-networks, value iteration, policy gradients, TRPO, AlphaGo Zero and more},
    year      = {2018},
    publisher = {Packt Publishing Ltd}
}

@inproceedings{Job2023TelemetriaAU,
  title={Telemetria Adaptativa Usando Aprendizado por Reforço Profundo em Redes Definidas por Software},
  author={Debora H. Job and Sidney C. de Lucena and Pedro Nuno Moura},
  booktitle={Brazilian Symposium on Computer Networks and Distributed Systems},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259756761}
}

@article{Kinoshita2022AprendizadoPR,
  title={Aprendizado por Reforço Profundo com Redes Recorrentes Aplicado {\`a} Negociaç{\~a}o do Minicontrato Futuro de D{\'o}lar},
  author={Jonathan Kenji Kinoshita and Douglas De Rizzo Meneghetti and Reinaldo Augusto da Costa Bianchi},
  journal={Anais do I Brazilian Workshop on Artificial Intelligence in Finance (BWAIF 2022)},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:250578214}
}

@article{Jesus2023AprendizadoPR,
  title={Aprendizado por Reforço Profundo para Navegaç{\~a}o Sem Mapa de um Ve{\'i}culo H{\'i}brido A{\'e}reo-Aqu{\'a}tico usando Imagens},
  author={Junior D. Jesus and Paulo Lilles Jorge Drews-Jr and Rodrigo da Silva Guerra},
  journal={Anais Estendidos do XV Simp{\'o}sio Brasileiro de Rob{\'o}tica e XX Simp{\'o}sio Latino-Americano de Rob{\'o}tica (SBR/LARS Estendido 2023)},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:267249421}
}

@article{Grando2021AprendizadoPR,
  title={Aprendizado por Reforço Profundo para Navegaç{\~a}o sem Mapa de um Ve{\'i}culo H{\'i}brido A{\'e}reo-Aqu{\'a}tico},
  author={Ricardo B. Grando and Paulo Lilles Jorge Drews-Jr},
  journal={Anais Estendidos do XIII Simp{\'o}sio Brasileiro de Rob{\'o}tica e XVIII Simp{\'o}sio Latino Americano de Rob{\'o}tica (SBR/LARS Estendido 2021)},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:246953635}
}

@article{Bezerra2021PropostaDC,
  title={Proposta de Controle de Espalhamento Espectral Utilizando Aprendizado por Reforço Profundo para Otimizaç{\~a}o do Desempenho de Redes LoRa/LoRaWAN},
  author={C. de S. Bezerra and Ant{\^o}nio Oliveira-Jr and Fl{\'a}vio Henrique Teles Vieira},
  journal={Anais da IX Escola Regional de Inform{\'a}tica de Goi{\'a}s (ERI-GO 2021)},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:245461569}
}

@article{Bezerra2023AprendizagemPR,
  title={Aprendizagem por Reforço Profundo com Redes Convolucionais Aplicada {\`a} Navegaç{\~a}o Aut{\^o}noma de Rob{\^o}s Reais Utilizando Treinamento em Cen{\'a}rio Virtual},
  author={Carlos Daniel Bezerra and Fl{\'a}vio Vieira},
  journal={Anais do XVI Congresso Brasileiro de Intelig{\^e}ncia Computacional},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:267512800}
}

@article{Lima2023AplicaoDM,
  title={Aplicaç{\~a}o do M{\'e}todo de Entropia Cruzada em Aprendizagem por Reforço para Controle de Fator de Espalhamento Espectral em Sistemas Internet das Coisas},
  author={Vittor Gomes Lima and Carlos Daniel Bezerra and Fl{\'a}vio Vieira},
  journal={Anais do XVI Congresso Brasileiro de Intelig{\^e}ncia Computacional},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:267534819}
}

@misc{pendyala2024solvingrealworldoptimizationproblem,
  title={Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering},
  author={Abhijeet Pendyala and Asma Atamna and Tobias Glasmachers},
  year={2024},
  eprint={2404.02577},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2404.02577}
}

@misc{han2018amberadaptivemultibatchexperience,
  title={AMBER: Adaptive Multi-Batch Experience Replay for Continuous Action Control},
  author={Seungyul Han and Youngchul Sung},
  year={2018},
  eprint={1710.04423},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1710.04423}
}

@article{Muller2023MultiAgentPP,
  title={Multi-Agent Proximal Policy Optimization for a Deadlock Capable Transport System in a Simulation-Based Learning Environment},
  author={Marcel M{\"u}ller and Tobias Reggelin and Hartmut Zadek and Lorena S. Reyes-Rubiano},
  journal={2023 Winter Simulation Conference (WSC)},
  year={2023},
  pages={1818-1829},
  url={https://api.semanticscholar.org/CorpusID:267338219}
}

@article{Li2022ResearchOO,
  title={Research on Obstacle Avoidance Strategy of Grid Workspace Based on Deep Reinforcement Learning},
  author={Shuo Li and Jun Zhang and Bin Zheng},
  journal={2022 2nd Asia-Pacific Conference on Communications Technology and Computer Science (ACCTCS)},
  year={2022},
  pages={11-15},
  url={https://api.semanticscholar.org/CorpusID:250663044}
}

@article{Tiong2022AutonomousVP,
  title={Autonomous Valet Parking with Asynchronous Advantage Actor-Critic Proximal Policy Optimization},
  author={Teckchai Tiong and Ismail Saad and Kenneth Tze Kin Teo and Herwansyah Bin Lago},
  journal={2022 IEEE 12th Annual Computing and Communication Workshop and Conference (CCWC)},
  year={2022},
  pages={0334-0340},
  url={https://api.semanticscholar.org/CorpusID:247231050}
}

@misc{azadeh2024advancesmultiagentreinforcementlearning,
  title={Advances in Multi-agent Reinforcement Learning: Persistent Autonomy and Robot Learning Lab Report 2024}, 
  author={Reza Azadeh},
  year={2024},
  eprint={2412.21088},
  archivePrefix={arXiv},
  primaryClass={cs.MA},
  url={https://arxiv.org/abs/2412.21088}
}

@misc{baheri2024synergyoptimaltransporttheory,
  title={The Synergy Between Optimal Transport Theory and Multi-Agent Reinforcement Learning}, 
  author={Ali Baheri and Mykel J. Kochenderfer},
  year={2024},
  eprint={2401.10949},
  archivePrefix={arXiv},
  primaryClass={cs.MA},
  url={https://arxiv.org/abs/2401.10949}
}

@inproceedings{Li2023RACEIM,
  title={RACE: Improve Multi-Agent Reinforcement Learning with Representation Asymmetry and Collaborative Evolution},
  author={Pengyi Li and Jianye Hao and Hongyao Tang and Yan Zheng and Xian Fu},
  booktitle={International Conference on Machine Learning},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:260872223}
}

@misc{wang2024multipleshipscooperativenavigation,
  title={Multiple Ships Cooperative Navigation and Collision Avoidance using Multi-agent Reinforcement Learning with Communication}, 
  author={Y. Wang and Y. Zhao},
  year={2024},
  eprint={2410.21290},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2410.21290}
}

@article{NiedziolkaDomanski2024AnEM,
  title={An environment model in multi-agent reinforcement learning with decentralized training},
  author={Rafał Niedzi{\'o}łka-Domański and Jarosław Bylina},
  journal={2024 19th Conference on Computer Science and Intelligence Systems (FedCSIS)},
  year={2024},
  pages={661-666},
  url={https://api.semanticscholar.org/CorpusID:273573410}
}

@misc{delafuente2024gametheorymultiagentreinforcement,
  title={Game Theory and Multi-Agent Reinforcement Learning: From Nash Equilibria to Evolutionary Dynamics}, 
  author={Neil De La Fuente and Miquel Noguer i Alonso and Guim Casadellà},
  year={2024},
  eprint={2412.20523},
  archivePrefix={arXiv},
  primaryClass={cs.MA},
  url={https://arxiv.org/abs/2412.20523}
}
@article{DeSouzaRibeiro2024AuxlioAD,
  title={Aux{\'i}lio ao diagn{\'o}stico de doenças emocionais e transtornos utilizando t{\'e}cnicas de aprendizado de m{\'a}quina},
  author={Artur De Souza Ribeiro and Wandr{\'e} Nunes de Pinho Veloso},
  journal={Programa de Iniciaç{\~a}o Cient{\'i}fica - PIC/UniCEUB - Relat{\'o}rios de Pesquisa},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:268181246}
}

@article{Brito2023AplicaesDA,
  title={Aplicaç{\~o}es de Aprendizado por Reforço em Manipuladores Rob{\'o}ticos: Uma Revis{\~a}o Sistem{\'a}tica},
  author={Frank Werlly Mendes de Brito and Andr{\'e} Luiz Carvalho Ottoni and Lara Toledo Cordeiro Ottoni},
  journal={Anais do XVI Congresso Brasileiro de Intelig{\^e}ncia Computacional},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:267519766}
}

@article{Felippe2024OUD,
  title={O uso de IA em ambientes de aprendizagem personalizados},
  author={Kellin Rangel Callegari Felippe and Deise Cordeiro de Souza and Herm{\'o}crates Gomes Melo J{\'u}nior and Jonathan Porto Galdino do Carmo and Marcos Antonio Soares de Andrade Filho and Renato Fernandes dos Santos and Rodrigo Rodrigues Pedra and Jocelino Ant{\^o}nio Demuner},
  journal={Caderno Pedag{\'o}gico},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269631777}
}

@article{Rocha2024SonsPE,
  title={Sons pulmonares e intelig{\^e}ncia artificial: uma scoping review das t{\'e}cnicas de aprendizado de m{\'a}quina aplicadas},
  author={Rodrigo Barbieri Rocha and Roberto Habermann Filho and Lucas dos Santos Batista and Geovana Maria Duarte and Marcos Roberto Torres J{\'u}nior},
  journal={REVISTA DELOS},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:274473157}
}

@article{Dias2023AplicaoDA,
  title={Aplicaç{\~a}o de AutoML em T{\'e}cnicas de Aprendizado de M{\'a}quina para Classificaç{\~a}o de Motoristas},
  author={Francisco {\'E}rbio Dias and Jos{\'e} Maria Pires Menezes Junior},
  journal={Anais do XVI Congresso Brasileiro de Intelig{\^e}ncia Computacional},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:267408732}
}

@article{Geremias2024OUD,
  title={O uso de Game Learning Analytics em Jogos Digitais Educacionais: Um Mapeamento Sistem{\'a}tico da Literatura},
  author={Matheus Soppa Geremias and Taynara Cerigueli Dutra and Eleandro Maschio and Isabela Gasparini},
  journal={Anais do XXXV Simp{\'o}sio Brasileiro de Inform{\'a}tica na Educaç{\~a}o (SBIE 2024)},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:274414399}
}

@misc{sukhbaatar2018learninggoalembeddingsselfplay,
  title={Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement Learning}, 
  author={Sainbayar Sukhbaatar and Emily Denton and Arthur Szlam and Rob Fergus},
  year={2018},
  eprint={1811.09083},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1811.09083}
}

@article{Brando2022MultiagentRL,
  title={Multiagent Reinforcement Learning for Strategic Decision Making and Control in Robotic Soccer Through Self-Play},
  author={Bruno Brand{\~a}o and Telma De Lima and Anderson Soares and Luckeciano Carvalho Melo and Marcos R.O.A. Maximo},
  journal={IEEE Access},
  year={2022},
  volume={10},
  pages={72628-72642},
  url={https://api.semanticscholar.org/CorpusID:250367384}
}

@article{Szemenyei2020LearningTP,
  title={Learning to Play Robot Soccer from Partial Observations},
  author={Marton Szemenyei and Patrik Reizinger},
  journal={2020 23rd International Symposium on Measurement and Control in Robotics (ISMCR)},
  year={2020},
  pages={1-6},
  url={https://api.semanticscholar.org/CorpusID:227219968}
}
@article{Cheng2022AuthenticBoundary,
  title={Authentic Boundary Proximal Policy Optimization},
  author={Yuan Cheng and Liang Huang and Xiaolin Wang},
  journal={IEEE transactions on cybernetics},
  year={2022},
  volume={52},
  number={9},
  pages={9428-9438},
  url={https://doi.org/10.1109/TCYB.2021.3051456}
}
@article{Jia2024ProximalPO,
  title={Proximal Policy Optimization with an Activated Policy Clipping},
  author={Lu Jia and Binglin Su and Du Xu and Yewei Wang},
  journal={2024 International Conference on Energy and Electrical Engineering (EEE)},
  year={2024},
  pages={1-5},
  url={https://api.semanticscholar.org/CorpusID:273377663}
}

@article{Liu2020OverviewOR,
  title={Overview of Reinforcement Learning Based on Value and Policy},
  author={Yunting Liu and Jia-ming Yang and Liang Chen and Ting Guo and Yu Jiang},
  journal={2020 Chinese Control And Decision Conference (CCDC)},
  year={2020},
  pages={598-603},
  url={https://api.semanticscholar.org/CorpusID:221118937}
}

@article{Takada2020ReinforcementLT,
  title={Reinforcement Learning to Create Value and Policy Functions Using Minimax Tree Search in Hex},
  author={Kei Takada and Hiroyuki Iizuka and Masahito Yamamoto},
  journal={IEEE Transactions on Games},
  year={2020},
  volume={12},
  pages={63-73},
  url={https://api.semanticscholar.org/CorpusID:68152112}
}

@article{Kim_2022,
   title={Efficient Off-Policy Safe Reinforcement Learning Using Trust Region Conditional Value At Risk},
   volume={7},
   ISSN={2377-3774},
   url={http://dx.doi.org/10.1109/LRA.2022.3184793},
   DOI={10.1109/lra.2022.3184793},
   number={3},
   journal={IEEE Robotics and Automation Letters},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Kim, Dohyeong and Oh, Songhwai},
   year={2022},
   month=jul, pages={7644–7651} 
}

@misc{yao2020smixlambdaenhancingcentralizedvalue,
  title={SMIX($\lambda$): Enhancing Centralized Value Functions for Cooperative Multi-Agent Reinforcement Learning}, 
  author={Xinghu Yao and Chao Wen and Yuhui Wang and Xiaoyang Tan},
  year={2020},
  eprint={1911.04094},
  archivePrefix={arXiv},
  primaryClass={cs.MA},
  url={https://arxiv.org/abs/1911.04094}, 
}

@misc{relay_long_horizon,
      title={Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning}, 
      author={Abhishek Gupta and Vikash Kumar and Corey Lynch and Sergey Levine and Karol Hausman},
      year={2019},
      eprint={1910.11956},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.11956}, 
}



@InProceedings{rSoccer,
author="Martins, Felipe B.
and Machado, Mateus G.
and Bassani, Hansenclever F.
and Braga, Pedro H. M.
and Barros, Edna S.",
editor="Alami, Rachid
and Biswas, Joydeep
and Cakmak, Maya
and Obst, Oliver",
title="rSoccer: A Framework for Studying Reinforcement Learning in Small and Very Small Size Robot Soccer",
booktitle="RoboCup 2021: Robot World Cup XXIV",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="165--176",
abstract="Reinforcement learning is an active research area with a vast number of applications in robotics, and the RoboCup competition is an interesting environment for studying and evaluating reinforcement learning methods. A known difficulty in applying reinforcement learning to robotics is the high number of experience samples required, being the use of simulated environments for training the agents followed by transfer learning to real-world (sim-to-real) a viable path. This article introduces an open-source simulator for the IEEE Very Small Size Soccer and the Small Size League optimized for reinforcement learning experiments. We also propose a framework for creating OpenAI Gym environments with a set of benchmarks tasks for evaluating single-agent and multi-agent robot soccer skills. We then demonstrate the learning capabilities of two state-of-the-art reinforcement learning methods as well as their limitations in certain scenarios introduced in this framework. We believe this will make it easier for more teams to compete in these categories using end-to-end reinforcement learning approaches and further develop this research area.",
isbn="978-3-030-98682-7"
}

@misc{framework_pequi_rSoccer,
    title = {Implementação do trabalho  Multiagent Reinforcement Learning for Strategic Decision Making and Control in Robotic Soccer Through Self-Play},
    year = {2024},
    url = {https://github.com/Pequi-Mecanico-SSL/RL},
    howpublished = {\url{https://github.com/Pequi-Mecanico-SSL/RL}}
}

@misc{robocin,
    title = {Site oficial da RobôCin},
    year = {2025},
    url = {https://www.robocin.com.br/},
    howpublished = {\url{https://www.robocin.com.br/}}
}

@misc{pequi_mecanico,
    title = {Site oficial do Pequi Mecânico},
    year = {2025},
    url = {https://www.pequi-mecanico.com.br/inicio},
    howpublished = {\url{https://www.pequi-mecanico.com.br/inicio}}
}


@misc{Classical_Robotics_Stack,
      title={Reinforcement Learning Within the Classical Robotics Stack: A Case Study in Robot Soccer}, 
      author={Adam Labiosa and Zhihan Wang and Siddhant Agarwal and William Cong and Geethika Hemkumar and Abhinav Narayan Harish and Benjamin Hong and Josh Kelle and Chen Li and Yuhao Li and Zisen Shao and Peter Stone and Josiah P. Hanna},
      year={2024},
      eprint={2412.09417},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2412.09417}
}


@article{soccer_skills_bipedal_robot,
doi = {10.1126/scirobotics.adi8022},
author = {Haarnoja, Tuomas and Moran, Ben and Lever, Guy and Huang, Sandy H. and Tirumala, Dhruva and Humplik, Jan and Wulfmeier, Markus and Tunyasuvunakool, Saran and Siegel, Noah Y. and Hafner, Roland and Bloesch, Michael and Hartikainen, Kristian and Byravan, Arunkumar and Hasenclever, Leonard and Tassa, Yuval and Sadeghi, Fereshteh and Batchelor, Nathan and Casarini, Federico and Saliceti, Stefano and Game, Charles and Sreendra, Neil and Patel, Kushal and Gwira, Marlon and Huber, Andrea and Hurley, Nicole and Nori, Francesco and Hadsell, Raia and Heess, Nicolas},
title = {Learning agile soccer skills for a bipedal robot with deep reinforcement learning},
journal = {Science Robotics},
volume = {9},
number = {89},
pages = {eadi8022},
year = {2024},
URL = {https://www.science.org/doi/abs/10.1126/scirobotics.adi8022},
eprint = {https://www.science.org/doi/pdf/10.1126/scirobotics.adi8022}
}


@inproceedings{ssl_skills,
  title={Learning Skills for Small Size League RoboCup},
  author={Schwab, Devin and Zhu, Yifeng and Veloso, Manuela M.},
  booktitle={Robot Soccer World Cup},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:52212489}
}


@article{curriculum,
  author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter},
  title = {Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey},
  year = {2020},
  issue_date = {January 2020},
  publisher = {JMLR.org},
  volume = {21},
  number = {1},
  issn = {1532-4435},
  journal = {J. Mach. Learn. Res.},
  month = jan,
  articleno = {181},
  numpages = {50},
  keywords = {transfer learning, reinforcement learning, curriculum learning}
}

@misc{PPO,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}
}

@article{curriculum_task_sequence,
  title={An Optimization Framework for Task Sequencing in Curriculum Learning},
  author={Francesco Foglino and Matteo Leonetti},
  journal={2019 Joint IEEE 9th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
  year={2019},
  pages={207-214},
  url={https://api.semanticscholar.org/CorpusID:59523682}
}


@INPROCEEDINGS{ppo_exploration_exploitation,
  author={Shen, Yuqing},
  booktitle={2024 4th International Conference on Computer, Control and Robotics (ICCCR)}, 
  title={Proximal Policy Optimization with Entropy Regularization}, 
  year={2024},
  pages={380-383},
  keywords={Training;Heuristic algorithms;Process control;Reinforcement learning;Entropy;Robustness;Trajectory;reinforcement learning;policy gradient;entropy regularization},
  doi={10.1109/ICCCR61138.2024.10585473}
}
@misc{openia_ppo_doc,
  title={OpenIA - Proximal Policy Optimization},
  author={OpenAI},
  year={2018},
  howpublished={\url{https://spinningup.openai.com/en/latest/algorithms/ppo.html}},
  note={Acessado em: 15 de outubro de 2024}
}

@misc{pytorch_ppo,
  title={Reinforcement Learning (PPO) with TorchRL Tutorial},
  author={PyTorch},
  year={2024},
  howpublished={\url{https://pytorch.org/rl/main/tutorials/coding_ppo.html}},
  note={Acessado em: outubro de 2024}
}

@misc{ppo_env_gym,
  title={Proximal Policy Optimization (PPO) for OpenAI Gym Environments using PyTorch},
  author={Verleysen, Niels},
  year={2023},
  howpublished={\url{https://github.com/VerleysenNiels/PPO-pytorch-gym}},
  note={Acessado em: outubro de 2024}
}

@misc{stable_baselines3,
  title={Stable Baselines3},
  author={DLR-RM},
  year={2019},
  howpublished={\url{https://github.com/DLR-RM/stable-baselines3}},
  note={Acessado em: outubro de 2024}
}

@inproceedings{seac_multiagent,
  author = {Christianos, Filippos and Sch\"{a}fer, Lukas and Albrecht, Stefano},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages = {10707--10717},
  publisher = {Curran Associates, Inc.},
  title = {Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/7967cc8e3ab559e68cc944c44b1cf3e8-Paper.pdf},
  volume = {33},
  year = {2020}
}

@article{review_cooperative_multi_agent,
  title={A review of cooperative multi-agent deep reinforcement learning},
  author={Afshin Oroojlooyjadid and Davood Hajinezhad},
  journal={Applied Intelligence},
  year={2019},
  volume={53},
  pages={13677-13722},
  url={https://api.semanticscholar.org/CorpusID:199543559}
}

@inproceedings{efficient_multi_agent,
 author = {Chang, Can and Mu, Ni and Wu, Jiajun and Pan, Ling and Xu, Huazhe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {12154--12168},
 publisher = {Curran Associates, Inc.},
 title = {E-MAPP: Efficient Multi-Agent Reinforcement Learning with Parallel Program Guidance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/4f2accafe6fa355624f3ee42207cc7b8-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@phdthesis{martins_vss,
  title={Exploring multi-agent deep reinforcement learning in IEEE very small size soccer},
  author={Martins, Felipe Bezerra},
  year={2023},
  school={Universidade Federal de Pernambuco}
}

@misc{survey_multi_agent,
      title={Multi-agent Reinforcement Learning: A Comprehensive Survey}, 
      author={Dom Huh and Prasant Mohapatra},
      year={2024},
      eprint={2312.10256},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2312.10256}
}

@phdthesis{agentes_inteligentes_puc,
  author = {Hyggo Oliveira de Almeida},
  title = {Agentes Inteligentes e Sistemas Multi-Agentes},
  school = {Pontifícia Universidade Católica do Rio de Janeiro},
  year = {2013},
  url = {https://www.maxwell.vrac.puc-rio.br/21194/21194_3.PDF}
}

@article{survey_self_play,
  title={Survey of Self-Play in Reinforcement Learning},
  author={DiGiovanni, Anthony and Zell, Ethan},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.02850},
  url={https://api.semanticscholar.org/CorpusID:235755071}
}

@misc{alpha_zero,
      title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}, 
      author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
      year={2017},
      eprint={1712.01815},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1712.01815}
}

@misc{survey_self_play_2024,
      title={A Survey on Self-play Methods in Reinforcement Learning}, 
      author={Ruize Zhang and Zelai Xu and Chengdong Ma and Chao Yu and Wei-Wei Tu and Shiyu Huang and Deheng Ye and Wenbo Ding and Yaodong Yang and Yu Wang},
      year={2024},
      eprint={2408.01072},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.01072}
}
@misc{self_play_adversarial_critic,
      title={Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models}, 
      author={Xiang Ji and Sanjeev Kulkarni and Mengdi Wang and Tengyang Xie},
      year={2024},
      eprint={2406.04274},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.04274}
}
@InProceedings{provable_self_play,
  title = 	 {Provable Self-Play Algorithms for Competitive Reinforcement Learning},
  author =       {Bai, Yu and Jin, Chi},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {551--560},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/bai20a/bai20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/bai20a.html}
}

@misc{huggingface_self_play,
  title={Self-Play: a classic technique to train competitive agents in adversarial games},
  author={Hugging Face},
  year={2024},
  note={Accessed in 2024},
  howpublished={\url{https://huggingface.co/learn/deep-rl-course/unit7/self-play}}
}

@misc{curriculum_learning_a_survey,
      title={Curriculum Learning: A Survey}, 
      author={Petru Soviany and Radu Tudor Ionescu and Paolo Rota and Nicu Sebe},
      year={2022},
      eprint={2101.10382},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.10382}, 
}


@InProceedings{curriculum_reinforcement_learning,
  title = 	 {Curriculum Reinforcement Learning via Constrained Optimal Transport},
  author =       {Klink, Pascal and Yang, Haoyi and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {11341--11358},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/klink22a/klink22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/klink22a.html},
  abstract = 	 {Curriculum reinforcement learning (CRL) allows solving complex tasks by generating a tailored sequence of learning tasks, starting from easy ones and subsequently increasing their difficulty. Although the potential of curricula in RL has been clearly shown in a variety of works, it is less clear how to generate them for a given learning environment, resulting in a variety of methods aiming to automate this task. In this work, we focus on the idea of framing curricula as interpolations between task distributions, which has previously been shown to be a viable approach to CRL. Identifying key issues of existing methods, we frame the generation of a curriculum as a constrained optimal transport problem between task distributions. Benchmarks show that this way of curriculum generation can improve upon existing CRL methods, yielding high performance in a variety of tasks with different characteristics.}
}


@inproceedings{diffusion_based_curriculum_reinforcement_learning,
  title={Diffusion-based Curriculum Reinforcement Learning},
  author={Erdi Sayar and Giovanni Iacca and Ozgur S. Oguz and Alois Knoll},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=yRhrVaDOWE}
}

@inproceedings{automatic_curriculum_learning_survey,
  author = {Portelas, R\'{e}my and Colas, C\'{e}dric and Weng, Lilian and Hofmann, Katja and Oudeyer, Pierre-Yves},
  title = {Automatic curriculum learning for deep RL: a short survey},
  year = {2021},
  isbn = {9780999241165},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
  articleno = {671},
  numpages = {7},
  location = {Yokohama, Yokohama, Japan},
  series = {IJCAI'20}
}

@misc{huggingface_curriculum,
    title = {(Automatic) Curriculum Learning for RL - Deep RL Course},
    author = {Hugging Face},
    year = {2024},
    url = {https://huggingface.co/learn/deep-rl-course/unitbonus3/curriculum-learning},
    howpublished = {\url{https://huggingface.co/learn/deep-rl-course/unitbonus3/curriculum-learning}},
    note = {Acessado em: março de 2024}
}

@inproceedings{curriculum_learning_in_rl,
  author    = {Sanmit Narvekar},
  title     = {Curriculum Learning in Reinforcement Learning},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {5195--5196},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/757},
  url       = {https://doi.org/10.24963/ijcai.2017/757}
}

@inproceedings{variational_curriculum_rl,
author = {Kim, Seongun and Lee, Kyowoon and Choi, Jaesik},
title = {Variational curriculum reinforcement learning for unsupervised discovery of skills},
year = {2023},
publisher = {JMLR.org},
abstract = {Mutual information-based reinforcement learning (RL) has been proposed as a promising framework for retrieving complex skills autonomously without a task-oriented reward function through mutual information (MI) maximization or variational empowerment. However, learning complex skills is still challenging, due to the fact that the order of training skills can largely affect sample efficiency. Inspired by this, we recast variational empowerment as curriculum learning in goal-conditioned RL with an intrinsic reward function, which we name Variational Curriculum RL (VCRL). From this perspective, we propose a novel approach to unsupervised skill discovery based on information theory, called Value Uncertainty Variational Curriculum (VUVC). We prove that, under regularity conditions, VUVC accelerates the increase of entropy in the visited states compared to the uniform curriculum. We validate the effectiveness of our approach on complex navigation and robotic manipulation tasks in terms of sample efficiency and state coverage speed. We also demonstrate that the skills discovered by our method successfully complete a real-world robot navigation task in a zero-shot setup and that incorporating these skills with a global planner further increases the performance.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {684},
numpages = {28},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{curml,
author = {Zhou, Yuwei and Chen, Hong and Pan, Zirui and Yan, Chuanhao and Lin, Fanqi and Wang, Xin and Zhu, Wenwu},
title = {CurML: A Curriculum Machine Learning Library},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548549},
doi = {10.1145/3503161.3548549},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {7359--7363},
numpages = {5},
keywords = {curriculum learning, machine learning, training strategy},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{boosted_curriculum_rl,
  title={Boosted Curriculum Reinforcement Learning},
  author={Klink, Pascal and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=anbBFlX1tJ1}
}

@misc{ssl_overview,
    title = {Technical Overview of the Small Size League},
    year = {2025},
    url = {https://ssl.robocup.org/technical-overview-of-the-small-size-league/},
    howpublished = {\url{https://ssl.robocup.org/technical-overview-of-the-small-size-league/}},
    note = {Acessado fev 2025}
}

@misc{turtlerabbit_tdp_2024,
    title = {TurtleRabbit 2024 SSL Team Description Paper},
    year = {2024},
    url = {https://ssl.robocup.org/wp-content/uploads/2024/04/2024_TDP_turtlerabbit.pdf},
    howpublished = {\url{https://ssl.robocup.org/wp-content/uploads/2024/04/2024_TDP_turtlerabbit.pdf}}
}

@misc{robocin_tdp_2024,
    title = {RobôCIn Small Size League Extended Team Description Paper for RoboCup 2024},
    year = {2024},
    url = {https://ssl.robocup.org/wp-content/uploads/2024/04/2024_ETDP_RoboCIn.pdf},
    howpublished = {\url{https://ssl.robocup.org/wp-content/uploads/2024/04/2024_ETDP_RoboCIn.pdf}}
}

@misc{orcabot_tdp_2024,
    title = {OrcaBOT Team Description Paper 2024},
    year = {2024},
    url = {https://ssl.robocup.org/wp-content/uploads/2024/04/2024_TDP_OrcaBOT.pdf},
    howpublished = {\url{https://ssl.robocup.org/wp-content/uploads/2024/04/2024_TDP_OrcaBOT.pdf}}
}

@misc{itandroids_tdp_2023,
    title = {ITAndroids Small Size League Team Description Paper for RoboCup 2023},
    year = {2023},
    url = {https://ssl.robocup.org/wp-content/uploads/2023/02/2023_TDP_ITAndroids.pdf},
    howpublished = {\url{https://ssl.robocup.org/wp-content/uploads/2023/02/2023_TDP_ITAndroids.pdf}}
}

@misc{robocup_ssl_brasil,
    title = {Repositório oficial da RoboCup SSL Brasil},
    year = {2024},
    url = {https://github.com/robocup-ssl-br},
    howpublished = {\url{https://github.com/robocup-ssl-br}}
}

