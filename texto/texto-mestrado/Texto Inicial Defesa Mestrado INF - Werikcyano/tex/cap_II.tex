\chapter{Fundamentação Teórica}
\label{cap:fund}

%% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Aprendizado por Reforço}
\label{sec:rl}

O Aprendizado por Reforço (RL) é uma abordagem de aprendizado baseada na interação entre um agente e seu ambiente, onde o objetivo principal é maximizar um sinal de recompensa acumulada ao longo do tempo \cite{sutton}. O agente aprende a tomar decisões através de tentativa e erro, ajustando suas ações com base no feedback recebido. Diferentemente do aprendizado supervisionado, que utiliza exemplos rotulados, o aprendizado por reforço explora a recompensa como único sinal de desempenho, lidando com a complexidade de recompensas atrasadas e incertezas na transição de estados. Formalmente, o RL é modelado através de \textbf{processos de decisão de Markov (MDP)}, que definem as interações em termos de estados, ações e recompensas, sendo amplamente aplicável a problemas de decisão sequencial em diversas áreas.

Dentre os métodos avançados de aprendizado por reforço, destaca-se o \textbf{Proximal Policy Optimization (PPO)}, que é amplamente utilizado devido à sua estabilidade em ambientes complexos. Quando aplicado ao \textbf{aprendizado por reforço multiagente (Multi-agent RL)}, permite que diversos agentes aprendam simultaneamente, interagindo de maneira colaborativa ou competitiva. Estratégias como o \textbf{self-play} têm mostrado grande eficácia ao permitir que agentes aprendam uns com os outros em cenários competitivos, como no futebol de robôs. Além disso, o \textbf{curriculum learning} tem sido utilizado para estruturar a aprendizagem progressiva, começando com tarefas simples e avançando para desafios mais complexos, um aspecto crucial em domínios como o \textbf{futebol de robôs}, onde os agentes precisam coordenar habilidades motoras e estratégias de equipe para alcançar um desempenho ótimo.

\subsection{Conceitos Básicos}
\label{subsec:rl_conceitos}

A interação entre o agente e o ambiente é representada esquematicamente na Figura \ref{fig/MDP.png}. O agente observa o estado atual \( S_t \) do ambiente e, com base em sua política de decisão, escolhe uma ação \( A_t \). Após a execução dessa ação, o ambiente evolui para um novo estado \( S_{t+1} \) e retorna ao agente uma recompensa \( R_{t+1} \) associada a essa transição.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{fig/MDP.png}
    \caption{Interação agente-ambiente no aprendizado por reforço. O agente toma decisões com base no estado atual \( S_t \) e recebe do ambiente uma recompensa \( R_{t+1} \) após realizar a ação \( A_t \).}
    \label{fig:agent_env_interaction}
\end{figure}

\subsubsection*{Elementos Fundamentais:}
\begin{enumerate}
    \item \textbf{Agente e Ambiente:} O agente é a entidade responsável por tomar decisões, enquanto o ambiente é tudo aquilo que responde às ações do agente e fornece feedback.
    \item \textbf{Política (\(\pi\)):} Define a estratégia do agente, especificando a probabilidade de selecionar uma ação específica \( A_t \) em um estado \( S_t \).
    \item \textbf{Sinal de Recompensa (\(R_{t+1}\)):} Indica o valor imediato recebido pelo agente após realizar uma ação. O objetivo é maximizar a soma acumulada das recompensas ao longo do tempo.
    \item \textbf{Função de Valor (\(v(s)\) e \(q(s, a)\)):} Estima o valor esperado da recompensa futura a partir de um estado \( s \) ou de um par estado-ação \( (s, a) \).
\end{enumerate}

\subsubsection*{Exploração vs. Exploitação:}
O dilema entre exploração e exploitação é um aspecto fundamental do aprendizado por reforço, especialmente em contextos de PPO e aprendizado multiagente \cite{sutton}. No PPO, este equilíbrio é gerenciado através da regularização de entropia e objetivos surrogate limitados, que previnem a convergência prematura para políticas subótimas enquanto mantêm a estabilidade do aprendizado. Em sistemas multiagente, este dilema torna-se ainda mais complexo devido à não-estacionariedade do ambiente, onde os agentes precisam explorar enquanto se adaptam aos comportamentos em evolução de outros agentes. Técnicas como aprendizado centralizado com execução descentralizada (CTDE) e modelagem de oponentes são empregadas para gerenciar eficientemente este compromisso, permitindo que os agentes compartilhem experiências de exploração enquanto mantêm estratégias eficazes de exploitação.

\subsubsection*{Modelagem por Processos de Decisão de Markov (MDPs)}

O aprendizado por reforço é frequentemente modelado por \textbf{Processos de Decisão de Markov} (MDPs, do inglês \textit{Markov Decision Processes}), uma estrutura matemática que captura os aspectos estocásticos e sequenciais da tomada de decisão. Um MDP é definido pelo conjunto quádruplo \((S, A, P, R)\), onde:

\begin{itemize}
    \item \(S\) é o conjunto de estados possíveis do ambiente.
    \item \(A\) é o conjunto de ações possíveis que o agente pode tomar.
    \item \(P(s'|s, a)\) é a probabilidade de transição para o estado \(s'\) dado o estado atual \(s\) e a ação \(a\) tomada.
    \item \(R(s, a)\) é a recompensa esperada ao tomar a ação \(a\) no estado \(s\).
\end{itemize}

O agente busca maximizar a soma esperada das recompensas acumuladas ao longo do tempo, definida pelo \textbf{retorno} \(G_t\). No caso de problemas de horizonte finito, o retorno é dado pela Equação \ref{eq:retorno_finito}:

\begin{equation}
\label{eq:retorno_finito}
G_t = \sum_{k=0}^{T} \gamma^k R_{t+k+1}
\end{equation}

No caso de tarefas contínuas, aplica-se a função de desconto, onde \(\gamma \in [0, 1]\) é o fator de desconto que controla o peso das recompensas futuras, como mostrado na Equação \ref{eq:retorno_infinito}:

\begin{equation}
\label{eq:retorno_infinito}
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{equation}

A modelagem dos MDPs requer duas funções principais de valor, que capturam a expectativa de recompensas futuras:

\paragraph{1. Função de valor de estado (\(v_\pi(s)\)):}
Define o valor esperado de se estar no estado \(s\) e seguir uma política \(\pi\) a partir desse estado, conforme a Equação \ref{eq:valor_estado}:

\begin{equation}
\label{eq:valor_estado}
v_\pi(s) = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s \right]
\end{equation}

Essa função pode ser definida recursivamente pela \textbf{equação de Bellman}, como mostrado na Equação \ref{eq:bellman_estado}:

\begin{equation}
\label{eq:bellman_estado}
v_\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s, a) \left[ R(s, a) + \gamma v_\pi(s') \right]
\end{equation}

\paragraph{2. Função de valor de ação (\(q_\pi(s, a)\)):}
Define o valor esperado de se tomar a ação \(a\) no estado \(s\) e seguir a política \(\pi\) posteriormente, como apresentado na Equação \ref{eq:valor_acao}:

\begin{equation}
\label{eq:valor_acao}
q_\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s, A_t = a \right]
\end{equation}

Também pode ser escrita de forma recursiva, conforme a Equação \ref{eq:bellman_acao}:

\begin{equation}
\label{eq:bellman_acao}
q_\pi(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a) + \gamma \sum_{a'} \pi(a'|s') q_\pi(s', a') \right]
\end{equation}

\paragraph{Equação de Bellman para o Ótimo:}
Para políticas ótimas \(\pi^*\), temos as funções de valor ótimo, dada pela Equação \ref{eq:bellman_otimo_estado}:

\begin{equation}
\label{eq:bellman_otimo_estado}
v^*(s) = \max_a \sum_{s'} P(s'|s, a) \left[ R(s, a) + \gamma v^*(s') \right]
\end{equation}

E a função de valor de ação ótima, apresentada na Equação \ref{eq:bellman_otimo_acao}:

\begin{equation}
\label{eq:bellman_otimo_acao}
q^*(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a) + \gamma \max_{a'} q^*(s', a') \right]
\end{equation}

Essas equações de Bellman são fundamentais para algoritmos de planejamento, como \textbf{Value Iteration} e \textbf{Policy Iteration}, mas quando a dinâmica do ambiente não é conhecida, métodos de aprendizado baseados em interação, como Q-learning, se tornam necessários.

Os MDPs constituem o núcleo matemático de muitos algoritmos de aprendizado por reforço, e são essenciais para o desenvolvimento de métodos de otimização de políticas, como o \textbf{Proximal Policy Optimization (PPO)}. Este algoritmo combina exploração e estabilidade ao ajustar políticas dentro de uma região de confiança definida.

\subsection{PPO (Proximal Policy Optimization)}
\label{subsec:ppo}
%O PPO foi introduzido formalmente em 2017 através do trabalho seminal de Schulman et al. \cite{https://arxiv.org/pdf/1707.06347}, que estabeleceu as bases teóricas e práticas deste algoritmo que viria a se tornar um dos métodos mais populares e eficazes no campo do aprendizado por reforço.

O Proximal Policy Optimization (PPO) representa um marco significativo no desenvolvimento de algoritmos de aprendizado por reforço, destacando-se pela sua combinação única de simplicidade de implementação, eficiência computacional e estabilidade durante o treinamento \cite{https://arxiv.org/abs/1707.06347}. Desenvolvido pela OpenAI, o PPO surgiu como uma evolução do Trust Region Policy Optimization (TRPO), introduzindo mecanismos mais eficientes para controlar a magnitude das atualizações de política \cite{https://spinningup.openai.com/en/latest/algorithms/ppo.html}.

O diferencial do PPO está em sua abordagem para otimização de políticas, que utiliza uma função objetivo substituta recortada (clipped surrogate objective) \cite{https://www.datacamp.com/pt/tutorial/proximal-policy-optimization}. Esta função limita efetivamente as mudanças na política, prevenindo atualizações muito grandes que poderiam desestabilizar o aprendizado. Matematicamente, a função objetivo do PPO é expressa como:

\begin{equation}
L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]
\end{equation}

onde $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ representa a razão entre as probabilidades das políticas nova e antiga, $A_t$ é a estimativa da vantagem, e $\epsilon$ é um hiperparâmetro que controla o tamanho máximo da atualização, tipicamente definido como 0.2 \cite{https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/}.

O PPO implementa uma arquitetura actor-critic, onde o actor é responsável pela política que mapeia estados para ações, enquanto o critic estima os valores dos estados para auxiliar no cálculo das vantagens \cite{https://pytorch.org/rl/main/tutorials/coding_ppo.html}. Esta estrutura dual permite um equilíbrio eficiente entre exploração e aproveitamento do conhecimento adquirido. O processo de treinamento ocorre em episódios, onde experiências são coletadas e utilizadas para atualizar tanto a política quanto a função de valor \cite{https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html}.

Uma característica fundamental do PPO é sua capacidade de lidar com espaços de ação tanto discretos quanto contínuos, tornando-o particularmente adequado para aplicações em robótica e controle \cite{https://petmec.uff.br/wp-content/uploads/sites/288/2021/09/Mexas_TCC2.pdf}. O algoritmo mantém um equilíbrio entre exploração e exploração através de um termo de entropia adicional na função objetivo, que incentiva a política a manter um nível apropriado de aleatoriedade nas decisões \cite{https://www.toolify.ai/pt/ai-news-pt/introduo-ao-proximal-policy-optimization-ppo-no-aprendizado-profundo-por-reforo-1059924}.

O PPO tem demonstrado resultados impressionantes em uma variedade de domínios complexos, desde jogos até tarefas de robótica \cite{https://github.com/VerleysenNiels/PPO-pytorch-gym}. Sua eficácia é particularmente notável em ambientes que requerem aprendizado contínuo e adaptação, como no caso do futebol de robôs, onde os agentes precisam constantemente ajustar suas estratégias em resposta a situações dinâmicas \cite{https://www.repositorio.unicamp.br/acervo/detalhe/1371987}.

No contexto do aprendizado multiagente, o PPO pode ser estendido para treinar múltiplos agentes simultaneamente, permitindo o desenvolvimento de comportamentos cooperativos ou competitivos \cite{https://pytorch.org/rl/0.6/tutorials/multiagent_ppo.html}. Esta característica é especialmente relevante para cenários de equipe, como o futebol de robôs, onde os agentes precisam coordenar suas ações para alcançar objetivos comuns \cite{https://codelabsacademy.com/pt/blog/proximal-policy-optimization-ppo-in-reinforcement-learning}.

\subsection{Multi-agent RL}
\label{subsec:marl}

O Aprendizado por Reforço Multiagente (MARL) representa uma extensão fundamental do RL tradicional, onde múltiplos agentes autônomos interagem em um ambiente compartilhado, aprendendo simultaneamente através de suas experiências coletivas. Como destacado em \cite{https://www.inf.ufrgs.br/~bazzan/downloads/cr_jai_masai_main_noh.pdf}, esta abordagem expande o paradigma clássico dos Processos de Decisão Markovianos (MDPs) para acomodar as complexidades inerentes às interações entre múltiplos tomadores de decisão.

A transição do RL single-agent para o MARL introduz desafios significativos, principalmente relacionados à não-estacionariedade do ambiente. Como observado em \cite{https://proceedings.neurips.cc/paper/2020/file/7967cc8e3ab559e68cc944c44b1cf3e8-Review.html}, quando múltiplos agentes atualizam suas políticas simultaneamente, o ambiente torna-se não-estacionário do ponto de vista de cada agente individual, violando premissas fundamentais do RL tradicional. Esta característica exige o desenvolvimento de algoritmos especializados que possam lidar com a natureza dinâmica das interações multiagente.

Um aspecto crucial do MARL, conforme destacado em \cite{https://www.semanticscholar.org/paper/A-Review-of-Cooperative-Multi-Agent-Deep-Learning-Oroojlooyjadid-Hajinezhad/f7c15e9ac6653330b7dd18a89301a3b333927db3}, é a distinção entre cenários cooperativos e competitivos. Em ambientes cooperativos, os agentes trabalham juntos para maximizar uma recompensa global compartilhada, enquanto em cenários competitivos, cada agente busca otimizar sua própria recompensa, potencialmente em detrimento dos outros.

A escalabilidade representa outro desafio significativo no MARL. De acordo com \cite{https://openreview.net/forum?id=CpnKq3UJwp}, o espaço de ações conjunto cresce exponencialmente com o número de agentes, tornando necessário o desenvolvimento de técnicas de decomposição e aproximação. Abordagens recentes, como o MAZero, têm demonstrado sucesso ao combinar planejamento baseado em modelo com técnicas de busca em árvore Monte Carlo para navegar eficientemente por estes espaços complexos.

Para lidar com estes desafios, diversas arquiteturas e algoritmos têm sido propostos. \cite{https://www.maxwell.vrac.puc-rio.br/21194/21194_3.PDF} destaca a importância de métodos como QMIX e MADDPG, que utilizam estruturas de valor decompostas e críticos centralizados para facilitar o aprendizado em ambientes multiagente. Estas abordagens permitem o treinamento centralizado com execução descentralizada, um paradigma que tem se mostrado particularmente eficaz em aplicações práticas.

A aplicação do MARL em domínios reais tem demonstrado resultados promissores. Como evidenciado em \cite{https://bdtd.ibict.br/vufind/Record/UFPE_b675a4ea6b1a1cb60a873b1d5bc3f9b2}, no contexto do futebol de robôs, abordagens multiagente superam significativamente métodos single-agent, especialmente em cenários que exigem coordenação complexa entre os membros da equipe. No entanto, o sucesso destas aplicações depende crucialmente da modelagem adequada das funções de recompensa e da implementação de mecanismos eficientes de comunicação entre agentes.

Desenvolvimentos recentes no campo, como destacado em \cite{https://arxiv.org/abs/2312.10256}, têm explorado a integração do MARL com outras tecnologias emergentes, como grandes modelos de linguagem e técnicas de aprendizado por demonstração. Estas integrações abrem novos caminhos para o desenvolvimento de sistemas multiagente mais sofisticados e adaptáveis, capazes de lidar com tarefas cada vez mais complexas em ambientes dinâmicos.

\subsection{Self-play}
\label{subsec:self_play}

O Self-play emergiu como uma técnica fundamental no aprendizado por reforço, permitindo que agentes artificiais desenvolvam habilidades avançadas através do treinamento contra versões de si mesmos. Como destacado em \cite{https://en.wikipedia.org/wiki/self-play}, esta abordagem permite que os agentes criem um currículo automático de aprendizado, onde enfrentam adversários progressivamente mais desafiadores, evitando assim a estagnação em estratégias subótimas.

A eficácia do self-play foi demonstrada de forma notável em diversos domínios complexos. Conforme documentado em \cite{https://pt.eitca.org/artificial-intelligence/eitc-ai-arl-advanced-reinforcement-learning/case-studies/classic-games-case-study/examination-review-classic-games-case-study/how-does-reinforcement-learning-through-self-play-contribute-to-the-development-of-superhuman-ai-performance-in-classic-games/}, sistemas como AlphaGo e AlphaZero alcançaram desempenho sobre-humano em jogos estratégicos complexos, aprendendo exclusivamente através da interação consigo mesmos.

Um aspecto crucial do self-play, como apontado em \cite{https://arxiv.org/html/2408.01072v1}, é sua capacidade de gerar dados de treinamento de alta qualidade sem necessidade de supervisão humana. Esta característica é particularmente valiosa em domínios onde exemplos de especialistas são escassos ou caros de obter. O processo permite que os agentes explorem o espaço de estratégias de forma mais abrangente do que seria possível com dados puramente supervisionados.

A implementação moderna do self-play frequentemente incorpora arquiteturas sofisticadas. De acordo com \cite{https://april.zju.edu.cn/core/papercite-data/pdf/liu2021spr.pdf}, o método SPAC (Self-Play Actor-Critic) introduziu inovações significativas ao integrar um crítico que considera as observações de ambos os agentes em ambientes competitivos, superando métodos tradicionais em eficácia.

No entanto, o self-play também apresenta desafios significativos. Como observado em \cite{https://proceedings.mlr.press/v119/bai20a.html}, existe o risco de convergência para equilíbrios subótimos, especialmente em jogos com múltiplos equilíbrios de Nash. Para mitigar este problema, técnicas como diversificação de oponentes e regularização de entropia têm sido empregadas com sucesso.

Desenvolvimentos recentes, descritos em \cite{https://openreview.net/forum?id=6smHoMdqhY}, têm explorado a integração do self-play com paradigmas emergentes como meta-aprendizado e modelos de linguagem grandes. Esta convergência tem aberto novos caminhos para o desenvolvimento de agentes mais adaptativos e versáteis.

A aplicação do self-play estende-se além dos jogos tradicionais. Como documentado em \cite{https://recodechinaai.substack.com/p/strawberry-o1-and-self-play-reinforcement}, a técnica tem sido aplicada com sucesso em domínios como robótica, simulações financeiras e sistemas de controle autônomo. Em cada caso, o self-play permite que os agentes desenvolvam estratégias robustas através da exploração sistemática do espaço de possibilidades.

O futuro do self-play parece promissor, com pesquisas contínuas focando na melhoria da eficiência computacional e na expansão de suas aplicações. Como sugerido em \cite{https://huggingface.co/learn/deep-rl-course/unit7/self-play}, novas técnicas de otimização e paralelização estão tornando o treinamento mais acessível, permitindo sua aplicação em uma gama cada vez maior de problemas práticos.

%% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Curriculum Learning}
\label{sec:curriculum}

O Curriculum Learning (CL) representa uma abordagem metodológica inspirada nos princípios pedagógicos humanos, onde o processo de aprendizagem é estruturado de forma progressiva, começando com tarefas mais simples e avançando gradualmente para desafios mais complexos. Esta metodologia tem demonstrado resultados significativos na otimização do treinamento de agentes de aprendizado, especialmente em contextos de Reinforcement Learning (RL), como destacado por \cite{https://arxiv.org/abs/2101.10382}.

\subsection{Conceitos Fundamentais}
\label{subsec:curriculum_conceitos}

O Curriculum Learning fundamenta-se em três componentes principais que trabalham em conjunto para estruturar o processo de aprendizagem. Como descrito em \cite{https://openreview.net/forum?id=anbBFlX1tJ1}, estes componentes são: geração de tarefas, sequenciamento e transferência de conhecimento.

A geração de tarefas envolve a criação sistemática de desafios intermediários que estabelecem uma ponte entre o estado inicial do agente e o objetivo final desejado. De acordo com \cite{https://proceedings.mlr.press/v162/klink22a/klink22a.pdf}, este processo pode incluir a modificação controlada de parâmetros ambientais, como a densidade de recompensas ou a complexidade dos estados apresentados ao agente.

O sequenciamento, por sua vez, representa a ordenação estratégica das tarefas geradas, garantindo uma progressão coerente e efetiva no processo de aprendizagem. Como destacado em \cite{https://jmlr.org/papers/volume21/20-212/20-212.pdf}, esta ordenação pode ser realizada através de métodos como Interpolação de Distribuições ou Optimal Transport, que otimizam a trajetória de aprendizado do agente.

A transferência de conhecimento, elemento crucial do CL, permite que o aprendizado adquirido em tarefas anteriores seja efetivamente utilizado para acelerar o domínio de novos desafios. \cite{https://openreview.net/forum?id=yRhrVaDOWE} demonstra que técnicas como o Boosted Curriculum RL (BCRL) podem aproximar funções de valor como somas de resíduos treinados incrementalmente, aumentando significativamente a capacidade expressiva do modelo.

Um aspecto fundamental do CL, conforme apresentado por \cite{https://zilliz.com/ai-faq/what-is-curriculum-learning-in-reinforcement-learning}, é a utilização de métricas de dificuldade para calibrar a progressão do currículo. Estas métricas podem incluir a entropia das políticas aprendidas ou a taxa de sucesso em submetas específicas, permitindo um ajuste dinâmico do processo de aprendizagem.

No contexto específico do Reinforcement Learning, o CL se integra naturalmente com os Processos de Decisão de Markov (MDPs), onde o agente interage com ambientes parametrizados de forma progressiva. Esta integração, como explicado em \cite{https://repositories.lib.utexas.edu/items/d6530e2e-35e5-452e-972c-8b3bb1dea93b}, permite uma acumulação estruturada de experiência, fundamental para o desenvolvimento de políticas robustas e eficientes.

\subsection{Aplicações em RL}
\label{subsec:curriculum_rl}

No contexto do Reinforcement Learning, o Curriculum Learning tem se mostrado particularmente eficaz em cenários complexos que apresentam desafios significativos para métodos tradicionais de aprendizagem. Como destacado por \cite{https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/}, a aplicação de currículos estruturados tem permitido avanços notáveis em domínios que exigem raciocínio hierárquico e planejamento de longo prazo.

Um exemplo significativo é encontrado em ambientes de navegação autônoma, onde \cite{https://huggingface.co/learn/deep-rl-course/unitbonus3/curriculum-learning} demonstra que agentes treinados com CL desenvolvem estratégias mais robustas ao serem expostos gradualmente a ambientes de complexidade crescente. O processo começa com cenários simplificados, como navegação em espaços abertos, e progride para ambientes com obstáculos dinâmicos e restrições temporais, resultando em políticas mais generalizáveis.

Em aplicações de robótica, \cite{https://arxiv.org/abs/2003.04960} apresenta uma abordagem onde o CL é utilizado para decompor tarefas motoras complexas em subcomponentes mais gerenciáveis. Os autores demonstram que esta decomposição hierárquica não apenas acelera o aprendizado, mas também melhora significativamente a qualidade das políticas aprendidas, especialmente em tarefas que requerem coordenação motora fina.

A integração do CL com técnicas de aprendizado multiagente tem produzido resultados promissores, como evidenciado por \cite{https://www.ijcai.org/proceedings/2017/757}. Em cenários competitivos e cooperativos, o uso de currículos adaptativos permite que os agentes desenvolvam estratégias sofisticadas através da exposição gradual a adversários ou parceiros de diferentes níveis de habilidade. Esta abordagem tem se mostrado particularmente eficaz em domínios como o futebol de robôs, onde a complexidade das interações entre agentes pode tornar o aprendizado direto impraticável.

Um aspecto crucial na aplicação do CL em RL, conforme destacado por \cite{https://arxiv.org/abs/2310.19424}, é a capacidade de automatizar a geração e adaptação de currículos. Técnicas recentes utilizam meta-aprendizado para otimizar a sequência de tarefas, permitindo que o currículo se ajuste dinamicamente ao progresso do agente. Esta automatização não apenas reduz a necessidade de intervenção humana, mas também permite a descoberta de sequências de treinamento não intuitivas que podem levar a um melhor desempenho.

A eficácia do CL em RL também se estende a domínios com espaços de estado contínuos e de alta dimensionalidade. \cite{https://dl.acm.org/doi/10.1145/3503161.3548549} demonstra como currículos bem projetados podem guiar a exploração em espaços complexos, reduzindo significativamente o tempo necessário para encontrar políticas ótimas. Esta abordagem tem se mostrado particularmente valiosa em aplicações industriais, onde o custo de exploração aleatória pode ser proibitivo.

\subsection{Estado da arte}
\label{subsec:curriculum_estado_arte}

O estado da arte em Curriculum Learning tem apresentado avanços significativos, particularmente na automatização do design de currículos e na integração com técnicas modernas de aprendizado profundo. Como destacado por \cite{https://proceedings.mlr.press/v162/klink22a/klink22a.pdf}, novas abordagens baseadas em Optimal Transport têm revolucionado a forma como os currículos são estruturados, permitindo uma transição mais suave entre diferentes níveis de complexidade.

Uma inovação notável é apresentada em \cite{https://openreview.net/forum?id=yRhrVaDOWE}, onde os autores introduzem um framework baseado em modelos de difusão para geração automática de tarefas intermediárias. Este método, denominado Diffusion-based Curriculum RL (DiCuRL), utiliza a modelagem probabilística para criar uma sequência contínua de objetivos que se adaptam ao progresso do agente, superando limitações tradicionais em ambientes complexos.

No contexto de sistemas multiagente, \cite{https://www.ijcai.org/proceedings/2017/757} apresenta avanços significativos na coordenação de currículos paralelos. Os autores demonstram como múltiplos agentes podem beneficiar-se de um currículo compartilhado, mantendo simultaneamente trajetórias de aprendizado individualizadas. Esta abordagem tem se mostrado particularmente eficaz em cenários competitivos e cooperativos, como simulações de mercado e jogos em equipe.

Desenvolvimentos recentes em meta-aprendizado, como descrito em \cite{https://arxiv.org/abs/2310.19424}, têm permitido a criação de currículos auto-adaptativos. Estes sistemas podem ajustar dinamicamente a dificuldade das tarefas baseando-se no desempenho do agente, utilizando métricas sofisticadas de progresso e transferência de conhecimento. Esta capacidade de adaptação automática representa um avanço significativo em relação aos currículos estáticos tradicionais.

\subsection{Vantagens e desafios}
\label{subsec:curriculum_vantagens_desafios}

A implementação do Curriculum Learning apresenta tanto benefícios substanciais quanto desafios significativos que precisam ser cuidadosamente considerados. Como observado por \cite{https://zilliz.com/ai-faq/what-is-curriculum-learning-in-reinforcement-learning}, as vantagens incluem uma aceleração significativa no processo de aprendizagem e uma melhoria na qualidade das políticas aprendidas.

Entre as principais vantagens, \cite{https://repositories.lib.utexas.edu/items/d6530e2e-35e5-452e-972c-8b3bb1dea93b} destaca a redução significativa no tempo de treinamento, com alguns experimentos mostrando uma diminuição de até 60% no número de iterações necessárias para convergência. Além disso, os autores observam uma melhoria na robustez das políticas aprendidas, com agentes demonstrando maior capacidade de generalização para cenários não vistos durante o treinamento.

No entanto, existem desafios importantes a serem considerados. Como apontado por \cite{https://openreview.net/forum?id=anbBFlX1tJ1}, um dos principais obstáculos é a definição apropriada de métricas de dificuldade para diferentes tipos de tarefas. A subjetividade inerente à noção de "dificuldade" pode tornar complexa a automatização completa do processo de design do currículo.

\cite{https://jmlr.org/papers/volume21/20-212/20-212.pdf} identifica outro desafio significativo: o balanceamento entre exploração e exploração durante o processo de aprendizagem. Currículos mal calibrados podem levar a uma convergência prematura para soluções subótimas ou, alternativamente, resultar em uma exploração excessiva que compromete a eficiência do treinamento.

Um aspecto particularmente desafiador, conforme destacado por \cite{https://dl.acm.org/doi/10.1145/3503161.3548549}, é a necessidade de recursos computacionais significativos para a implementação efetiva de currículos adaptativos. O custo de gerar e validar sequências de treinamento personalizadas pode ser proibitivo para algumas aplicações, especialmente em domínios que requerem simulações complexas ou processamento em tempo real.

Apesar destes desafios, o consenso na literatura, como evidenciado por \cite{https://arxiv.org/abs/2101.10382}, é que os benefícios do Curriculum Learning geralmente superam suas limitações, especialmente em domínios complexos onde abordagens tradicionais de aprendizado por reforço mostram-se inadequadas. A contínua evolução de técnicas automatizadas para design de currículos e a crescente disponibilidade de recursos computacionais sugerem um futuro promissor para esta abordagem.

%% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Futebol de Robôs}
\label{sec:futebol_robos}

O futebol de robôs representa uma das principais plataformas para pesquisa e desenvolvimento em robótica móvel e sistemas multiagentes. Dentro deste contexto, a RoboCup Small Size League Entry Level (SSL-EL) emerge como uma categoria especialmente projetada para facilitar a entrada de novas equipes, mantendo os desafios técnicos fundamentais do futebol de robôs enquanto simplifica aspectos complexos da competição \cite{https://robocup-ssl.github.io/ssl-rules/sslrules.html}.

\subsection{Visão geral}
\label{subsec:futebol_visao}

A SSL-EL foi estabelecida como uma divisão de entrada da Small Size League, visando democratizar o acesso à competição de futebol de robôs. Diferentemente da divisão principal, a SSL-EL opera com 6 robôs por equipe em um campo reduzido, permitindo que equipes iniciantes desenvolvam suas habilidades técnicas e estratégicas de forma progressiva \cite{https://github.com/robocup-ssl-br/rules-ssl-el}.

O ambiente de jogo é estruturado em torno de um sistema centralizado de visão (SSL-Vision), que fornece informações em tempo real sobre as posições dos robôs e da bola através de câmeras suspensas \cite{https://ssl.robocup.org/technical-overview-of-the-small-size-league/}. Esta arquitetura permite que as equipes foquem no desenvolvimento de estratégias de controle e coordenação, sem a necessidade inicial de implementar sistemas complexos de percepção local.

Os robôs da SSL-EL, embora sujeitos a restrições dimensionais similares às da divisão principal (diâmetro ≤18 cm, altura ≤15 cm), podem ser construídos com soluções mais acessíveis. A equipe TurtleRabbit, por exemplo, demonstrou a viabilidade de construir robôs competitivos com orçamento reduzido, utilizando componentes comerciais e estruturas impressas em 3D \cite{https://ssl.robocup.org/wp-content/uploads/2024/04/2024_TDP_turtlerabbit.pdf}.

A competição promove desafios técnicos específicos, como o Ball Placement Challenge, que permitem às equipes desenvolverem e testarem capacidades fundamentais de forma isolada \cite{https://ssl.robocup.org/robocup-2024/robocup-2024-technical-challenges/}. Esta abordagem gradual ao desenvolvimento de habilidades tem se mostrado efetiva para a evolução das equipes, como evidenciado pelos resultados de competições recentes \cite{https://ssl.robocup.org/robocup-2022-technical-challenges/}.

\subsection{Desafios específicos}
\label{subsec:futebol_desafios}

A SSL-EL apresenta desafios únicos que equilibram complexidade técnica com acessibilidade. Um dos principais desafios é a coordenação multiagente em tempo real, onde os robôs devem operar de forma sincronizada em velocidades consideráveis, tomando decisões em milissegundos \cite{https://www.cs.cmu.edu/~robosoccer/small/}. Esta coordenação torna-se ainda mais complexa devido aos ruídos de percepção e latência de comunicação inerentes ao sistema.

O aspecto financeiro representa outro desafio significativo para equipes iniciantes. No entanto, iniciativas como a da equipe TurtleRabbit demonstram que é possível desenvolver soluções competitivas com orçamento limitado \cite{https://ssl.robocup.org/wp-content/uploads/2024/04/2024_TDP_turtlerabbit.pdf}. Através do uso de componentes comerciais acessíveis e técnicas de manufatura como impressão 3D, equipes podem construir robôs funcionais mantendo os custos controlados.

O processamento em tempo real das informações do SSL-Vision apresenta desafios técnicos específicos \cite{https://ssl.robocup.org/technical-overview-of-the-small-size-league/}. As equipes precisam desenvolver sistemas robustos para lidar com:
- Fusão de dados de múltiplas câmeras
- Compensação de falhas momentâneas na detecção
- Filtragem de ruídos e correção de discrepâncias
- Predição de movimentos para compensar latências

A implementação de estratégias de jogo efetivas também representa um desafio significativo. Como documentado pela equipe RoboCIn \cite{https://ssl.robocup.org/wp-content/uploads/2024/04/2024_ETDP_RoboCIn.pdf}, é necessário desenvolver algoritmos sofisticados para navegação, controle de bola e coordenação tática, mesmo no contexto simplificado da SSL-EL.

\subsection{Trabalhos relacionados}
\label{subsec:futebol_trabalhos}

Diversos trabalhos têm contribuído para o avanço da SSL-EL, focando em diferentes aspectos da competição. A equipe OrcaBOT, por exemplo, desenvolveu uma abordagem inovadora para o design de robôs de baixo custo, documentada em seu TDP (Team Description Paper) \cite{https://ssl.robocup.org/wp-content/uploads/2024/04/2024_TDP_OrcaBOT.pdf}, demonstrando a viabilidade de construir sistemas competitivos com recursos limitados.

No campo do controle e estratégia, trabalhos significativos têm emergido da comunidade acadêmica. A UFPE, através de seu programa de mestrado, tem explorado a aplicação de técnicas de aprendizado por reforço no contexto do futebol de robôs \cite{https://www.youtube.com/watch?v=GCwrTJuuJ3w}. Estas pesquisas demonstram o potencial de algoritmos de aprendizado de máquina para desenvolver estratégias de jogo mais sofisticadas.

A organização da RoboCup tem contribuído significativamente através da documentação e padronização de regras e especificações técnicas \cite{https://robocup-ssl.github.io/ssl-rules/sslrules.html}. Este trabalho de documentação tem sido fundamental para permitir que novas equipes ingressem na competição com uma compreensão clara dos requisitos e desafios.

Iniciativas de código aberto, como as disponibilizadas pela comunidade brasileira de robótica \cite{https://github.com/robocup-ssl-br}, têm facilitado o desenvolvimento de novas equipes. Estes recursos incluem implementações de referência para sistemas de controle, simuladores e ferramentas de desenvolvimento, permitindo que equipes iniciantes construam sobre uma base sólida de conhecimento estabelecido.