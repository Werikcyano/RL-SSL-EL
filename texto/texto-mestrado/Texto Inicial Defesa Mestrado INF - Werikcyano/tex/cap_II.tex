\chapter{Fundamentação Teórica}
\label{cap:fund}

%% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Aprendizado por Reforço}
\label{sec:rl}

O Aprendizado por Reforço (Reinforcement Learning - RL) é uma área do aprendizado de máquina que se concentra em agentes que aprendem a tomar decisões através da interação com um ambiente. Diferente do Aprendizado Supervisionado e do Aprendizado Não Supervisionado, o RL depende principalmente de feedback em forma de recompensas obtidas através das interações que o agente realiza no ambiente.

\subsection{Fundamentos e conceitos básicos de Aprendizado por Reforço}
\label{subsec:rl_fund}

O Aprendizado por Reforço é estruturado em torno de conceitos fundamentais que estabelecem como agentes podem aprender através da interação com o ambiente. Esta área combina elementos de programação dinâmica, teoria de controle e aprendizado estatístico para desenvolver algoritmos que permitem que agentes aprendam comportamentos ótimos através de tentativa e erro.

Os principais componentes do Aprendizado por Reforço são:

\begin{itemize}
\item \textbf{Agente}: A entidade que aprende e toma decisões. Ele observa o estado atual do ambiente, escolhe ações para executar e recebe feedback na forma de recompensas.

\item \textbf{Ambiente}: O mundo com o qual o agente interage. Ele fornece observações ao agente e responde às suas ações, gerando novos estados e recompensas.

\item \textbf{Estado}: A representação da situação atual do ambiente. O estado pode ser completamente ou parcialmente observável pelo agente.

\item \textbf{Ação}: As possíveis decisões que o agente pode tomar em cada estado. O conjunto de ações pode ser discreto ou contínuo.

\item \textbf{Recompensa}: O feedback numérico que o agente recebe após cada ação. É um sinal escalar que indica o quão boa foi uma ação tomada em um determinado estado. O objetivo do agente é maximizar a soma das recompensas ao longo do tempo.

\item \textbf{Política}: A estratégia que o agente usa para escolher ações com base no estado atual. Pode ser determinística ou estocástica.
\end{itemize}

O processo de aprendizagem no RL pode ser descrito pela seguinte sequência:

\begin{enumerate}
\item O agente observa o estado atual do ambiente.
\item Com base nesse estado, o agente escolhe uma ação.
\item O ambiente transita para um novo estado como resultado da ação.
\item O agente recebe uma recompensa associada à transição.
\item O agente atualiza sua política de decisão com base na experiência adquirida.
\end{enumerate}

Este processo é frequentemente modelado como um Processo de Decisão de Markov (MDP).

Existem alguns principais abordagens para o RL, incluindo:

\begin{itemize}
\item \textbf{Métodos baseados em valor}: Aprendem a função valor-ação \textbf{Q(s,a)}.
\item \textbf{Métodos baseados em política}: Otimizam diretamente a política \textbf{$\pi$}.
\item \textbf{Métodos ator-crítico}: Combinam estimativas de valor e otimização de política.
\end{itemize}

\subsubsection{Processo de Decisão de Markov (MDP)}
\label{subsubsec:mdp}

Os Processos de Decisão de Markov (MDPs) são um modelo matemático fundamental no aprendizado por reforço, utilizado para modelar situações onde é necessário tomar decisões sequenciais em ambientes com incerteza \cite{introducao_modelos_probabilisticos}. A Figura \ref{fig:mdp_ilustracao} ilustra um exemplo de MDP. Os MDPs possuem as seguintes características principais:

\begin{itemize}
\item \textbf{Estados (S)}: Representam as possíveis situações do ambiente.
\item \textbf{Ações (A)}: Conjunto de decisões que o agente pode tomar em cada estado.
\item \textbf{Função de transição P(s'|s,a)}: Probabilidade de transição para o estado s' dado que a ação a foi tomada no estado s.
\item \textbf{Função de recompensa R(s,a,s')}: Retorno numérico recebido ao realizar a transição de s para s' tomando a ação a.
\item \textbf{Fator de desconto γ}: Valor entre 0 e 1 que representa a importância de recompensas futuras.
\end{itemize}

Um MDP é formalmente definido como uma tupla (S, A, P, R, γ), onde:

\begin{itemize}
\item S é o conjunto de estados
\item A é o conjunto de ações
\item P : S × A × S → [0, 1] é a função de transição
\item R : S × A × S → ℝ é a função de recompensa
\item γ ∈ [0, 1] é o fator de desconto
\end{itemize}

O objetivo em um MDP é encontrar uma política ótima π* que maximize o retorno esperado:

$$ π* = \arg\max_π \mathbb{E}\left[\sum_{t=0}^{\infty} γ^t R(s_t, a_t, s_{t+1}) | π\right] $$

onde π : S → A é uma política que mapeia estados para ações.

\begin{figure}[H]
 \centering
 \includegraphics[width=0.60\textwidth]{@MDP.png}
 \caption{Exemplo de um Processo de Decisão de Markov (MDP).}
 \label{fig:mdp_ilustracao}
\end{figure}

Uma característica fundamental dos MDPs é a propriedade de Markov, que estabelece que a probabilidade de transição para um novo estado depende apenas do estado atual e da ação tomada, não dependendo de estados ou ações anteriores \cite{sutton}.

Os MDPs são amplamente utilizados em diversas áreas, incluindo:

\begin{itemize}
\item Robótica e controle automático
\item Planejamento de trajetórias
\item Gestão de recursos
\item Sistemas de recomendação
\item Jogos e simulações
\end{itemize}

Existem várias extensões dos MDPs para lidar com diferentes cenários:

\begin{itemize}
\item \textbf{POMDPs (Partially Observable MDPs)}: Lidam com situações onde o estado não é completamente observável.
\item \textbf{LLMDPs (Language-Limited MDPs)}: Incorporam restrições de linguagem nas ações e observações consideradas \cite{introducao_modelos_probabilisticos}.
\end{itemize}

Os MDPs fornecem uma base teórica sólida para o desenvolvimento de algoritmos de aprendizado por reforço, permitindo a modelagem e solução de problemas complexos de tomada de decisão sequencial sob incerteza.

\subsubsection{Funções de valor e política}
\label{subsubsec:valor_politica}

As funções de valor e política são conceitos fundamentais no aprendizado por reforço (RL), desempenhando papéis cruciais na tomada de decisões e na avaliação de estratégias. As funções de valor estimam o quão vantajoso é para um agente estar em um determinado estado ou executar uma ação específica em um estado. Existem dois tipos principais de funções de valor: a Função de Valor de Estado (V-function) e a Função de Valor de Ação (Q-function).

A Função de Valor de Estado, representada como \( V^\pi(s) \), indica o valor esperado de estar em um estado específico seguindo uma política \(\pi\). Matematicamente, é definida como \( V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] \), onde \( G_t \) é o retorno acumulado a partir do tempo \( t \). Por outro lado, a Função de Valor de Ação, representada como \( Q^\pi(s,a) \), indica o valor esperado de tomar uma ação específica em um estado específico e depois seguir uma política \(\pi\). Sua definição matemática é \( Q^\pi(s,a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a] \).

Essas funções de valor são essenciais para métodos baseados em valor, como Q-learning e DQN (Deep Q-Network) \cite{0fe19b1631b60807bffb7757865b184d797c3016}. Elas permitem que o agente avalie a qualidade das ações em diferentes estados, facilitando a tomada de decisões ótimas.

As funções de política, por sua vez, definem o comportamento do agente, mapeando estados para ações. Existem dois tipos principais de políticas: a Política Determinística e a Política Estocástica. A Política Determinística mapeia cada estado para uma única ação, representada como \(\pi(s) = a\). Já a Política Estocástica mapeia cada estado para uma distribuição de probabilidade sobre as ações, representada como \(\pi(a|s) = P(A_t = a | S_t = s)\).

As funções de política são centrais para métodos baseados em política, como Policy Gradient e PPO (Proximal Policy Optimization) \cite{0fe19b1631b60807bffb7757865b184d797c3016}. Elas permitem que o agente aprenda diretamente qual ação tomar em cada estado, sem necessariamente estimar valores de estado ou ação.

A relação entre funções de valor e política é estreita. A função de valor avalia a qualidade de uma política, enquanto a política pode ser derivada da função de valor, por exemplo, escolhendo a ação com o maior valor \( Q \) em cada estado. Alguns métodos, como os algoritmos ator-crítico, combinam ambos os conceitos: o "ator" (política) decide quais ações tomar, e o "crítico" (função de valor) avalia essas ações e fornece feedback para melhorar a política \cite{0fe19b1631b60807bffb7757865b184d797c3016}.

O objetivo do RL é encontrar a política ótima \(\pi^*\) que maximize o retorno esperado. Isso pode ser alcançado de várias maneiras: métodos baseados em valor aprendem a função de valor ótima e derivam a política ótima a partir dela \cite{54807143779fa04466a0b0d01f4d1ecc94eb39ae}; métodos baseados em política otimizam diretamente a política, muitas vezes usando gradientes de política \cite{0fe19b1631b60807bffb7757865b184d797c3016}; e métodos híbridos combinam aprendizado de valor e otimização de política, como em algoritmos ator-crítico \cite{1911.04094}.

Recentes avanços incluem o uso de redes neurais profundas para aproximar funções de valor e política em ambientes complexos, como demonstrado no jogo de Hex \cite{54807143779fa04466a0b0d01f4d1ecc94eb39ae}, e o desenvolvimento de métodos robustos para lidar com incertezas e riscos em ambientes desafiadores \cite{2312.00342}.

As funções de valor e política são elementos cruciais no aprendizado por reforço (RL), especialmente em algoritmos avançados como o Proximal Policy Optimization (PPO). No PPO, as funções de valor são utilizadas para estimar a vantagem, que é essencial para o processo de otimização da política. A política, por sua vez, é ajustada para maximizar uma função objetivo surrogate, garantindo que as atualizações sejam estáveis e eficientes. Essa interação entre funções de valor e política permite que o PPO aprenda de forma eficaz em ambientes complexos, equilibrando desempenho e estabilidade. Vamos explorar esses conceitos e sua relação com o PPO.


\subsection{Aprendizado por Reforço Profundo}
\label{subsec:deep_rl}

\subsubsection{Redes neurais como aproximadores de função}
\label{subsubsec:redes_neurais}

\subsection{Proximal Policy Optimization (PPO)}
\label{subsec:ppo}

\subsubsection{Motivação e princípios do PPO}
\label{subsubsec:ppo_principios}

\subsubsection{Função objetivo e mecanismo de clipping}
\label{subsubsec:ppo_objetivo}

\subsubsection{Vantagens do PPO em ambientes contínuos e multiagentes}
\label{subsubsec:ppo_vantagens}

%% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Aprendizado por Reforço Multiagente}
\label{sec:marl}

\subsection{Desafios em Ambientes Multiagente}
\label{subsec:desafios_multi}

\subsubsection{Coordenação e competição entre agentes}
\label{subsubsec:coordenacao}

\subsubsection{Não-estacionariedade do ambiente}
\label{subsubsec:nao_estacionariedade}

\subsection{Aplicações em Futebol de Robôs}
\label{subsec:futebol_robos}

\subsubsection{Visão geral da categoria SSL-EL}
\label{subsubsec:ssl_el}

\subsubsection{Desafios específicos do domínio}
\label{subsubsec:desafios_dominio}

%% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Técnicas de Aprendizado Progressivo}
\label{sec:aprendizado_prog}

\subsection{Curriculum Learning}
\label{subsec:curriculum}

\subsubsection{Conceito e motivação}
\label{subsubsec:curriculum_conceito}

\subsubsection{Desenho de currículos para RL}
\label{subsubsec:curriculum_desenho}

\subsubsection{Aplicações em robótica e jogos}
\label{subsubsec:curriculum_aplicacoes}

\subsection{Self-Play}
\label{subsec:self_play}

\subsubsection{Princípios do self-play em RL}
\label{subsubsec:self_play_principios}

\subsubsection{Geração automática de currículos via self-play}
\label{subsubsec:self_play_curriculos}

\subsubsection{Exemplos de sucesso em jogos complexos}
\label{subsubsec:self_play_exemplos}

%% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Integração de Técnicas para Aquisição Progressiva de Habilidades}
\label{sec:integracao}

\subsection{Combinando Curriculum Learning e Self-Play}
\label{subsec:combinacao}

\subsubsection{Sinergias entre as abordagens}
\label{subsubsec:sinergias}

\subsubsection{Desafios na integração}
\label{subsubsec:desafios_integracao}

\subsection{Aplicação ao Futebol de Robôs Multiagente}
\label{subsec:aplicacao_futebol}

\subsubsection{Desenho de currículos para habilidades de futebol}
\label{subsubsec:curriculos_futebol}

\subsubsection{Transição do curriculum para self-play competitivo}
\label{subsubsec:transicao_self_play}

\subsubsection{Potenciais benefícios na aquisição de habilidades complexas}
\label{subsubsec:beneficios_aquisicao}