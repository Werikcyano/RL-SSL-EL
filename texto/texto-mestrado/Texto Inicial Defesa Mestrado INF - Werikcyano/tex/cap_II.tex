\chapter{Fundamentação Teórica}
\label{cap:fund}

%% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Aprendizado por Reforço}
\label{sec:rl}

O Aprendizado por Reforço (AR) é uma área do Aprendizado de Máquina que se concentra em como agentes de software devem tomar ações em um ambiente para maximizar alguma noção de recompensa cumulativa \cite{sutton}. 

\subsection{Conceitos básicos}
\label{subsec:rl_conceitos}

O AR envolve diversos conceitos fundamentais que formam a base de seu funcionamento:

\begin{itemize}
    \item \textbf{Agente}: É a entidade que aprende e toma decisões. O agente interage com o ambiente, observando estados e executando ações \cite{sutton}.
    
    \item \textbf{Ambiente}: Representa o mundo no qual o agente interage e aprende. O ambiente pode ser determinístico ou estocástico, totalmente ou parcialmente observável.
    
    \item \textbf{Estado}: Define a situação atual do ambiente. O estado pode ser representado de várias formas, como um vetor de características ou uma imagem.
    
    \item \textbf{Ação}: Uma decisão tomada pelo agente que pode afetar o estado do ambiente. As ações podem ser discretas (como mover para cima/baixo/esquerda/direita) ou contínuas (como aplicar uma força específica).
    
    \item \textbf{Recompensa}: Um feedback numérico do ambiente após uma ação. A recompensa indica o quão desejável é o resultado imediato da ação.
    
    \item \textbf{Política}: A estratégia que o agente usa para determinar a próxima ação com base no estado atual. A política pode ser determinística ou estocástica.
\end{itemize}

Além disso, existem outros conceitos importantes como:

\begin{itemize}
    \item \textbf{Função valor}: Uma previsão da recompensa futura esperada, que pode ser:
    \begin{itemize}
        \item Função valor-estado (V): Estima o valor esperado de estar em um determinado estado
        \item Função valor-ação (Q): Estima o valor esperado de tomar uma ação específica em um determinado estado
    \end{itemize}
    
    \item \textbf{Modelo do ambiente}: Uma representação interna de como o ambiente funciona, podendo ser baseado em modelo ou livre de modelo
    
    \item \textbf{Exploração vs. Aproveitamento}: O dilema entre explorar novas ações para obter mais informações sobre o ambiente e aproveitar o conhecimento atual para maximizar a recompensa
\end{itemize}

O objetivo principal do AR é que o agente aprenda uma política ótima que maximize a recompensa total ao longo do tempo. A formalização matemática do AR geralmente usa Processos de Decisão de Markov (MDP), que fornecem um framework para modelar a tomada de decisão em situações onde os resultados são parcialmente aleatórios e parcialmente sob o controle do agente \cite{mcmc}.

\subsection{PPO (Proximal Policy Optimization)}
\label{subsec:ppo}

\subsection{Multi-agent RL}
\label{subsec:marl}

\subsection{Self-play}
\label{subsec:self_play}

%% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Curriculum Learning}
\label{sec:curriculum}

\subsection{Conceitos fundamentais}
\label{subsec:curriculum_conceitos}

\subsection{Aplicações em RL}
\label{subsec:curriculum_rl}

\subsection{Estado da arte}
\label{subsec:curriculum_estado_arte}

\subsection{Vantagens e desafios}
\label{subsec:curriculum_vantagens_desafios}

%% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Futebol de Robôs}
\label{sec:futebol_robos}

\subsection{Visão geral}
\label{subsec:futebol_visao}

\subsection{Desafios específicos}
\label{subsec:futebol_desafios}

\subsection{Trabalhos relacionados}
\label{subsec:futebol_trabalhos}